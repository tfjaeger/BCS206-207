---
title: 'Data Wrangling and Visualization 101 in R'
author: "Florian Jaeger"
date: "Fall 2020"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
urlcolor: blue
header-includes:
   - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, results = "markup", warning = TRUE, cache = FALSE,
  fig.align = "center", fig.width = 6.5)
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("R.matlab")   # for interacting with MatLab
library("readxl")     # for interacting with Excel
library("cowplot")    # for combining figures
library("plotly")     # for interactive plots

theme_set(theme_bw())
```


# Overview

This document provides an introduction to data wrangling and visualization in R. It contains sections marked as exercises. **Please complete these exercises prior to next Monday's class** and submit them on slack. In addition your homework for this Wednesday is to apply the knowledge gain from this tutorial to *your own data*. **By this Wednesday, post at least one plot about your data on slack using the skills learned in this tutorial.** You are encouraged to ask questions (and help others) on slack. 

# Preliminaries

## Version control

RStudio makes version control, data backup, and data sharing easy (e.g., via Github.com). To use it, download and install git on your computer. Get a free github.com or bitbucket.com account. You only have to do this once.

Then, for each project, create a new project in RStudio and link it to the remote repository (select "Create project" > "Version control"). You will have to enter a URL for the remote repository, which you get, for example, at github.com under the repository’s main page by clicking the "Clone or download button".

For step by step instructions, follow these links:

 * [Setting up RStudio for version control](http://www.geo.uzh.ch/microsite/reproducible_research/post/rr-rstudio-git/)
 * [RStudio help on version control](https://support.rstudio.com/hc/en-us/articles/200532077?version=1.1.442&mode=desktop)
 * [Reverting a file to an earlier version](https://stackoverflow.com/questions/38465025/how-do-i-revert-to-a-previous-git-in-r-studio)

You can clone this repository from Github. In RStudio:

 1. From the file menu (top left), select "New project …"
 1. Select "Version control"
 1. Select "Git"
 1. Enter "\url{https://github.com/tfjaeger/BCS206DataWrangling.git}" in the top field (for the Repository URL). Select where you want this project to be stored in the last/third field. I usually just go with the default, which for me is the desktop.
 1. This should clone the tutorial for Monday from Github.com to your local drive, and open it as a new R project.
 1. Once that has happened, check in the Git tab (top right of RStudio window) whether you’re on the right branch of the repository. 
 1. **This tutorial is the 2020-2021 branch of the git repository.** Right next to the "New Branch" button it should say "2020-2021". If it instead says "master", click on "master". This opens a menu from which you can select "2020-2021". Please do so.


## Reproducibility and literate coding

R and RStudio support reproducibility oriented literate coding via Sweave and Knitr: lab books, presentations, and papers can weave/knit together data, code, and text. The document you share contains the code needed to create its outputs (figures, tables, etc.). This is achieved by combining latex or R markdown with R code (or, for that matter, code from other programming languages). For an excellent video-based introduction, see this [tutorial on R markdown](https://rmarkdown.rstudio.com/lesson-1.html). *This document is R markdown compiled with RStudio's knitr.* If you downloaded the [git repository](https://github.com/tfjaeger/BCS206DataWrangling), you should see both the .Rmd and the .pdf file. The latter is created ("knitted") from the former. The former contains all the R code required to generate the figures, etc. in the PDF file.



# Data wrangling

The *R* libraries *dplyr* provide us with efficient ways to transform ('wrangle') our data tables. The library *magrittr* let's us concatenate these operations in transparent and easy to read code. Like many of the packages we use in this tutorial, they are part of the *tidyverse*---an awesome and powerful collection of R packages for the data sciences (\url{https://www.tidyverse.org/}).

There are many video and interactive tutorials for R, RStudio, and the tidyverse online. I *highly* recommend that you invest a handful of ours to get your inner data scientist started. Your future self will thank you. R is increasingly used both in academia and in the data sciences, and that includes the libraries we use for this tutorial (which are also available in similar form for Python). I would recommend going through at least the first four Sections of [RStudio's R Primers](https://rstudio.cloud/learn/primers): The basics, working with data, visualizing data, and tidying data.

## An example data set

```{r, include=FALSE}
set.seed(9876)

# Let's make some fake data
d = 
  crossing(
    size = c("1","2","4"),
    condition = c("easy", "hard"),
    trial = 1:240,
    subject = 1:10
  ) %>%
  mutate(
    muLogOddsCorrect = 0 + as.numeric(size) * .25 + (as.numeric(size) - 1) * ifelse(condition == "easy", 0, 1) * .5,
    muLogRT = 6.5 - as.numeric(size) * .6 - as.numeric(size) * ifelse(condition == "easy", 0, 1) * -.16) %>%
  group_by(subject) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, 1.5),
    muLogRT.bySubject = rnorm(1, 0, 0.3)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(exp(rnorm(1, muLogRT + muLogRT.bySubject - .2 * correct, .1)), 3)
  ) %>%
  as_tibble() %>%
  select(-starts_with("mu")) %>%
  mutate_at(c("size", "condition", "subject"), factor)

write_csv(d, path = "./data/example_data.csv")
```

We will illustrate the use of *dplyr* with the following data from an experiment with a 2AFC task. On each trial, subjects had to fixate on a fixation cross. The fixation cross was then replaced by a small small number, and subjects had to quickly answer whether they saw the number "3" or "8". Half of the trials displayed "3", half displayed "8". Within subject, the experiment manipulated two variables, the size of the number (1, 2, or 4) and whether the number was surrounded by four other numbers above, below, left, and right of it (hard) or not (easy). The presence of other numbers presented close to the target number is known to make recogntion of the target number harder---an effect known as "visual crowding". For each trial, the software recorded whether the subject's response was correct (1 = correct; 0 = incorrect) and the reaction time for the response (RT):

```{r, echo=TRUE}
summary(d)

glimpse(d)
```


## Dplyr's verbs
Dplyr has 'verbs' like filter, select, summarize, mutate, transmute, etc. to let use conduct operations on our data, and reshape the data frame into the format we need. We can use dplyr, for example, to calculate the proportion correct answers in our experiment by using *summarise*. 

```{r, echo=T}
summarise(d, meanCorrect = mean(correct))
```

Or just for number size 1:

```{r, echo=T}
d.1 = filter(d, size == "1")
summarise(d.1, meanCorrect = mean(correct))
```

## Maggritr's pipes

Here we will use only of the 'pipes' magrittr provides:

 * x %>% f: takes x and hands it to the function f on the right, as f's first argument
 * x %<>% f1 %>% f2 %>% etc.: takes x hands it to f1, takes the output of f1 and hands it to f2, etc. And since the first pipe was %<>% (rather than just %>%), the final result will be written back into x.

![Magritt's pipe](./figures/otherpipe.jpeg){width=150px}

![Magrittr's pipe](./figures/pipe.png){width=150px}

## Putting it together: Wrangling through pipes

Remember how we got the mean proportion correct for just number size 1?
```{r, echo=T}
d.1 = filter(d, size == "1")
summarise(d.1, meanCorrect = mean(correct))
```

This is inelegant and hard to read. Pipes let us make this more transparent:

```{r, echo=T}
d %>%
  filter(size == "1") %>%
  summarise(meanCorrect = mean(correct))
```

And this advantage becomes even clearer, the more operationgs we concatenate. For example, *group_by* is an elegant operator that tells the pipes to conduct all subsequent operations for each of the groups (and then put all the separate outcomes back together into a single data frame). So if we want the proportion correct for all groups:

```{r, echo=T}
d %>%
  group_by(size) %>%
  summarise(meanCorrect = mean(correct))
```

## Exercises

Remember [dplyr cheatsheet](https://dplyr.tidyverse.org/)! How can we: 

 * View the entire data set? (*View*)
 
 * Calculate the by-subject averages for all three number sizes? (*group_by*, *summarise*)
 
 * Calculate the by-subject standard deviations around those averages? (*group_by*, *summarise*)
 
 * Attach this information (the averages and SDs) to each row of the present data.frame? (*group_by*, *mutate*)
 
 * Determine whether RTs were on average faster for correct, as compared to incorrect, trials?
 
 * Add a column for log-transformed RTs to the data set?
 
 * Remove the old column for raw RTs? (*select*)
 
 * Sort the data by log-transformed reaction times? (*arrange*)

Say we further have an additional data frame with information about our subjects:

```{r joining, echo=FALSE}
# Let's make some fake subject data.
d.subj = d %>%
  select(subject) %>%
  distinct() %>%
  rowwise() %>%
  mutate(
    gender = factor(ifelse(rbinom(1,1,.5) == 1, "female", "male")),
    age = round(rnorm(1, 20, 1), 0)
  )

print(d.subj)
```  

 * How can we join the information from the two data sources together? (*left_join*)


```{r}
d.subj
d %<>%
  left_join(d.subj)

d
```




# Data visualization

The two main libraries in R we will be using for visualization are *ggplot2* and *plotly*. Ggplot2 provides a grammar of graphics approach to plotting. Plotly let's us interact with our data. In particular, *ggplotly()* wrapped around a ggplot2 figure let's us interact with that figure. Before you start, make sure to get the superb [ggplot2 cheatsheet](https://ggplot2.tidyverse.org/).

## Ggplot2's components (aesthetic mappings)

In order to plot in ggplot2, we need to understand the way it thinks about visualization. There are excellent online course that explain all of this, so I focus on the basics. **For reasons that become apparent below, we start by just looking at one subject, subject 1.**

At the heart of a plot is a mapping between properties of your data (i.e., column in your data frame) and abstract properties of the plot (such as x- or y-coordinates, color, fill, transparency (alpha), linetype, shape, or label information). If we call the function ggplot() in order to create a figure, we specify two arguments: the name of the data frame we want to work with, and the mapping. The latter is done through a helpful function called *aes()*---for aesthetics:

```{r, echo=T, fig.height=3}
d.s1 = d %>%
  filter(subject == 1)

ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT))

# or equivalently and shorter:
# ggplot(
#   d,
#   aes(
#       x = size,
#       y = RT))
```

### Adding geometric components (geoms)

Notice that this by itself only returns an empty plot. That's the case because we have not yet specified how we want the abstract properties of the graph to be expressed visually. That's achieved by specifying *geom*s (for geometrics), such as points (*geom_point*), lines (*geom_line*), histograms (*geom_histogram*), lineranges (*geom_linerange*), and many similar functions (I take it you're getting th hang for the naming scheme ...). You can find all of them on the [ggplot2 cheatsheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf). 

We add such components to a plot with "+". 

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT)) +
  geom_point()

plot(p)
```
Notice that this plot is not particularly helpful! There is a lot of points and they are all overlaid, making it hard to get a clear idea of the data. One way we can improve this plot is to add transparency to the points. Another way is to jitter the points along the x-axis. Both approaches help convey information about the distribution of RTs at each number size, and sometimes---especially, when we have a lot of data---we might want to combine both approaches. For example, here's the same plot with just transparency added. Since we have quite a bit of data, it still does not convey much information:


```{r, echo=T, fig.height=3}
p = ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT)) +
  geom_point(alpha = .05)

plot(p)
```

And here is the plot with some additional jitter added along the x-axis. Note that we intentionally avoid jitter along the y-axis:

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT)) +
  geom_point(alpha = .05, position = position_jitter(height = 0))

plot(p)
```
That's much better already, giving us a much clearer idea of how the data is distributed! We are now in a position to think about how to include information about condition in this plot. There are a few ways to do this. For example, we could use different point shapes or colors for the easy and hard condition. Here, we do the latter:

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT,
      color = condition)) +
  geom_point(alpha = .05, position = position_jitter(height = 0))

plot(p)
```
Notice the clear 'stripes' in the data


### Visualizing data summaries

While it is often useful to see the raw data, this *alone* can be insufficient to communicate important aspects of the data. Specifically, we often are interested in whether the distributions of two conditions (e.g., the number size and easy vs. hard conditions) differ. The plots we have seen so far *suggest* that this might be the case, but we can't be certain. In order to convey information about the distribution of our data, we often provide data *summaries* instead of---or in addition to---the raw data. Whenever possible, I would highly recommend to go with the "in addition to" option. There is a lof of value in seeing the raw data, as long as the overall tendencies we are interested in can also be recognized.

Data can be summarized down to various levels of abstraction. For example, we can *bin* the data, and report the binned 'raw' data: 

```{r, fig.height=3}
ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT,
      fill = condition)) +
  geom_dotplot(binaxis = "y", stackdir = "center", color = NA, dotsize = .5, binwidth = 10)
```
Dotplots can be an effective means for a categorical outcome variable (or the proportions resulting from them). Try it out for plotting the correctness of responses, rather than RTs. For a continuous outcome variables, however, dotplots are difficult to read (as in the RT plot above), and you might be better of with plotting the histogram or density of the distribution. This can elegantly be done by adding so-called violins to the raw data:

```{r, fig.height=3}
p +
  geom_violin(fill = NA, size = 1)
```
Some researchers also like dotplots, which try to provide a high-level summary of the data while also providing *some* information about the general distribution of the data and any potential outliers. Boxplots tend to be plotted *instead* of the raw data since boxplots already contain any raw data that is not captured well by the summary they provide (the outlier points). The line in the center of the box provides the median of the data in that condition. The upper and lower end of the box indicate the "inter-quartile range (IQR) from the 25th to the 75th quantile of the outcome (you can use the command quantiles() to get the quantiles yourself, if you want to compare them to what the plot provides). The whiskers (the lines below and above the box) extend to the largest and smallest value at most 1.5-times larger/smaller than the end of the box. Just as the height of the box provides a summary of the middle 50\% of the data (the IQR), the whiskeys give an idea of the distribution beyond that middle part. Any points outside of that range are plotted and are referred to as "outliers". Notably all outliers in the present case are high RT outliers, not low RT outliers. This is quite common for variables that have highly skewed distributions: RTs have a lower, but no upper bound. 

Finally, the notches (indentation on the sides of the box) provide the 95\% confidence intervals around the means (CIs. We'll learn later *how CIs are obtained*, and what exactly they mean. For now, it suffices to say that (appropriately obtained) CIs are a critical visual component in plots when we want to know whether the means differ between conditions.

```{r, fig.height=3}
ggplot(
  data = d.s1,
  mapping = 
    aes(
      x = size,
      y = RT,
      color = condition)) +
  geom_boxplot(fill = NA, outlier.alpha = .1, notch = T)
```
In publications, you will find that the data is often summarized even further into just the central tendency (mean) and its 95\% CIs. Here we plot the mean and 95\% CIs on top of the raw data. In this case, we have so much data that the CIs are very narrow and not even recognizable in the plot:

```{r}
p +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = .2)
```
Here have used a pointrange to summarize the data. Alternatives include linerange (geom_linerange; no mean shown) or bar graphs with error intervals (geom_bar and geom_errorbar).

**It is important that we summarize the data at an appropriate level when obtained CIs.** We will cover this in more detail in a later class, but for now remember that what we do here is ok if we're looking at the data from one subject, but that we need to do things a bit differently when we look at the combined data from multiple subjects. 


### Plotting data from multiple subjects

So far we have plotted just one subject. Notice what happens when we update the scatter plot with the data from *all* subjects. There are now multiple 'stripes' visible in the data. Think about why that is the case. Any ideas?

```{r, echo=T, fig.height=3}
# We can update a plot with new data using %+%
p = p %+% 
  d

plot(p)
```
Correct: subjects differ from each other. This includes how fast they are overall, and might also include differences in how they are affected by the experimental conditions. This also means that the individual observations we are *not* independent of each other because the data points from the same subject resemble each other. We talk about "repeated measures" from the subject and call such data "repeated measures data". Another term that is used to refer to such data is *hierarchical* data or *hierarchically organized() data. Whenever we deal with such data, we need to be careful, for example, when we calculate CIs. We will return to that issue in our class on confidence intervals. For now, the next section focuses on how we can effectively visualize the data for all subjects without having to make many individual plots. 


### Facets

If we want to have separate panels conditional on another variable, we can do so through *facet*s. There are two major facet functions, *facet_wrap* (to have panels conditional on one variable) and *facet_grid* (conditional on two variables). For example, we can have separate panels for each subject:

```{r, fig.height=12, fig.width=8}
p +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = .2) +
  facet_wrap(facets = ~ subject)
```

Or we could show by-subject RTs in two columns, separately for false and correct answers. Here, we do so after first sampling 6 random subjects (since the plot would otherwise be rather large).

```{r, fig.height=8, fig.width=4}
p %+%
  (d %>% filter(subject %in% sample(levels(subject), 6))) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = .2) +
  facet_grid(facets = subject ~ correct, labeller = label_both)
```


### Scales and coordinate systems

Sometimes we don't want to see all of the data, or we want to zoom into some ranges of our data. We can do so by explicitly specifying the x- and y-limits of our coordinate system:


```{r, fig.height=3}
p +
  geom_violin(fill = NA) +
  coord_cartesian(ylim = c(200,1500))
```

Note that this zooms into parts of our data without excluding any data (e.g., from the calculation of the violins, which have the same shape as above). If we want to exclude data, transform data or in other ways change the way the aesthetical mappings are interpreted, this is achieved through *scales*. For example, the following *ex*cludes all RTs below 300 and above 2000. Note how that changes the violin plots (as it should: they estimate the ditribution of RTs):

```{r, fig.height=3}
p +
  geom_violin(fill = NA) +
  scale_y_continuous(limits = c(200,1500))
```
More generally, **scales are applied before *ggplot2* applies any statistics, whereas coordinate systems do only affect how we see the data**. This is important to remember. For example, since reaction times often have distributions that are more lognormal, rather than normal, let's update our original plot to use a log-transformed y-axis. This does not only affect how we *see* the data, it also affects how teh densities that are plotted by the violin geom are estimated:

```{r, fig.height=3}
p = p +
  geom_violin(fill = NA) +
  scale_y_log10()

plot(p)
```


## Pipes (again)

Of course, we can use pipes to pipe the data frame into the plotting function, optionally after first piping the data through some additional *dplyr* operations (since the output of that entire pipe is again a data frame):

```{r, echo = T, fig.height=3}
p = d %>%
  filter(size != "1") %>%
  ggplot(
    aes(
      x = size,
      y = RT,
      color = condition)) +
  geom_point(alpha = .05, position = position_jitter()) 

plot(p)
```

## Exercises

Remember [ggplot2 cheatsheet](https://ggplot2.tidyverse.org/)!

 * Plot a histogram of the RTs by number size. Make on version where you plots the histograms in different facets, and another version where you have only one facet and use fill color to distinguish between number sizes. (*geom_histogram*)
 * Plot the average proportion of correct answers by number size as a pointrange.
 * Do the same, but first average by subject and number size, and then plot the average (and confidence interval) of those by-subject averages of correct responses.
 * Try to make a pie chart that shows the proportion correct for the three number sizes. (*coord_polar*)







# Working out the details of your visualizations (Next week)

 * making x-axis have right units
 * color choices
 * axis labels
 * legends
 * panel grid, etc.
 
## Themes

Themes can be used to customize any detail of a plot. There are many customized themes, including in additional libraries (e.g., *ggthemes*). Here a few examples:

```{r, echo=T, fig.height=3, fig.width=4}
library(ggthemes)

p + theme_wsj()
p + theme_economist()
p + theme_tufte()
# etc.
```

If fact, any ggplot has a theme, even if you don't specify one. In that case, *ggplot2* uses the default theme. You can also create your own themes, or modify any aspect of the present theme. Check out *?theme for more information. For example, to remove the x-axis labels, and to change the font of the y-axis labels:

```{r, echo=T, fig.height=3, fig.width=4}
p + theme(
  axis.text.x = element_blank(),
  axis.text.y = element_text(family = "Times", face = "bold", size = 6))
```

Elements that can be modified or removed include any text elements in the axis or legend (alignment, angle, size, font, emphasis, etc.), the fill or border lines of any element (incl. the background color of the panel or the entire plot area), the padding, margins, and other spacing between the elements, any tick marks on the axis, any grid lines, any aspects of the legend its orientation, shape, etc. It's worth looking into the helpfile or a *ggplot2* overview, as the themes also contain many elements that are not even visible in the default.

## Other libraries

There's a huge number of additional libraries that build on *ggplot2*, and they keep changing. Many of the libraries introduce additional geoms, incl. single-line commands for complex plots (e.g., $k$-by-$k$ correlation plots; trees diagrams; field-specific plot types like map types, dentrograms, ridge plots, 3D plots). Other packages extend what *ggplot2* can do or make it easier to use. 

Here I just note a few:

 * Extend functionality:
  * gganimate for animations, movies, gifs, etc.
  * plotly to turn any ggplot into an interactive graph
  * cowplot to combine multiple ggplots into a single figure
  
 * Make it easier to do things that ggplot can already do:
   * ggforce to add powerful functionality
   * ggthemes to add themes
   * gggraph to visualize graphs
   * ggeffects to help with plotting model summaries
   * ggsignif to add significance brackets (seen in many publications)
   * ggpubr to make ggplots publication ready
   * ggsci to add scientifically themed palettes
   * viridis to add highly contrastive color schemes (beautiful)

 * Shiny to make web-based apps in which users can interact with your plots.

For further inspiritation, check out these galaries:

 * [Gallery of R plots](https://www.r-graph-gallery.com/)
 * [List of ggplot2 extensions](https://exts.ggplot2.tidyverse.org/gallery/)

For example, instead of faceting our data by subject, we might animate it by subject. Gganimate can create movies, but here I am embedding the animation directly into the PDF. This will only work if you work the document in Acrobat Reader. Try it out:

```{r animation, fig.show='animate', interval=1/10, fig.width=4, fig.height=3}
library(gganimate)

p +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = .2) +
  transition_states(subject) +
  labs(title = paste("Subject {ceiling(frame/ 10)} of {ceiling(nframes / 10)}")) +
  enter_fade() +
  exit_fade()
```
 

# Session info
```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
