---
title: 'Data Wrangling and Visualization 101 in R'
author: "for BCS 206"
date: "Fall 2020"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, results = "markup", warning = FALSE, cache = FALSE,
  fig.align = "center", fig.width = 6.5)
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("R.matlab")   # for interacting with MatLab
library("readxl")     # for interacting with Excel
library("cowplot")    # for combining figures
library("plotly")     # for interactive plots

theme_set(theme_bw())
```


# Preliminaries

## Version control

RStudio makes version control, data backup, and data sharing easy (e.g., via Github.com). To use it, download and install git on your computer. Get a free github.com or bitbucket.com account. You only have to do this once.

Then, for each project, create a new project in RStudio and link it to the remote repository (select “Create project” > “Version control”). You will have to enter a URL for the remote repository, which you get, for example, at github.com under the repository’s main page by clicking the “Clone or download button”.

For step by step instructions, see:

 * [Setting up RStudio for version control](http://www.geo.uzh.ch/microsite/reproducible_research/post/rr-rstudio-git/)
 * [RStudio help on version control](https://support.rstudio.com/hc/en-us/articles/200532077?version=1.1.442&mode=desktop)
 * [Reverting a file to an earlier version](https://stackoverflow.com/questions/38465025/how-do-i-revert-to-a-previous-git-in-r-studio)


## Reproducibility and literate coding

R and RStudio support reproducibility oriented literate coding via Sweave and Knitr: lab books, presentations, and papers can weave/knit together data, code, and text. The document you share contains the code needed to create its outputs (figures, tables, etc.). This is achieved by combining latex or R markdown with R code (or, for that matter, code from other programming languages). For an excellent video-based introduction, see this [tutorial on R markdown](https://rmarkdown.rstudio.com/lesson-1.html). *This document is R markdown compiled with RStudio's knitr.



# Data wrangling

The *R* libraries *dplyr* provide us with efficient ways to transform ('wrangle') our data tables. The library *magrittr* let's us concatenate these operations in transparent and easy to read code. 

## An example data set

```{r, include=FALSE}
# Let's make some fake data
d = 
  crossing(
    size = c("1","2","4"),
    condition = c("easy", "hard"),
    trial = 1:240,
    subject = 1:10
  ) %>%
  mutate(
    muLogOddsCorrect = 0 + as.numeric(size) * .25 + (as.numeric(size) - 1) * ifelse(condition == "easy", 0, 1) * .5,
    muLogRT = 6.5 - as.numeric(size) * .6 - as.numeric(size) * ifelse(condition == "easy", 0, 1) * -.33) %>%
  group_by(subject) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, 1.5),
    muLogRT.bySubject = rnorm(1, 0, 0.3)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(exp(rnorm(1, muLogRT + muLogRT.bySubject - .2 * correct, .05)), 3)
  ) %>%
  as_tibble() %>%
  select(-starts_with("mu")) %>%
  mutate_at(c("size", "condition", "subject"), factor)
```

We will illustrate the use of *dplyr* with the following data from an experiment with a 2AFC task in three within-subject letter sizes (A, B, C), for which we have extracted correctness (1 = correct; 0 = incorrect) and reaction times (RT):

```{r, echo=TRUE}
summary(d)

glimpse(d)
```


## Dplyr's verbs
Dplyr has 'verbs' like filter, select, summarize, mutate, transmute, etc. to let use conduct operations on our data, and reshape the data frame into the format we need. We can use dplyr, for example, to calculate the proportion correct answers in our experiment by using *summarise*. 

```{r, echo=T}
summarise(d, meanCorrect = mean(correct))
```

Or just for letter size 1:

```{r, echo=T}
d.A = filter(d, size == "1")
summarise(d.A, meanCorrect = mean(correct))
```

## Maggritr's pipes

Here we will use only of the 'pipes' magrittr provides:

 * x %>% f: takes x and hands it to the function f on the right, as f's first argument
 * x %<>% f1 %>% f2 %>% etc.: takes x hands it to f1, takes the output of f1 and hands it to f2, etc. And since the first pipe was %<>% (rather than just %>%), the final result will be written back into x.

![Magritt's pipe](./figures/otherpipe.jpeg){width=150px}

![Magrittr's pipe](./figures/pipe.png){width=150px}

## Putting it together: Wrangling through pipes

Remember how we got the mean proportion correct for just letter size 1?
```{r, echo=T}
d.A = filter(d, size == "1")
summarise(d.A, meanCorrect = mean(correct))
```

This is inelegant and hard to read. Pipes let us make this more transparent:

```{r, echo=T}
d %>%
  filter(size == "1") %>%
  summarise(meanCorrect = mean(correct))
```

And this advantage becomes even clearer, the more operationgs we concatenate. For example, *group_by* is an elegant operator that tells the pipes to conduct all subsequent operations for each of the groups (and then put all the separate outcomes back together into a single data frame). So if we want the proportion correct for all groups:

```{r, echo=T}
d %>%
  group_by(size) %>%
  summarise(meanCorrect = mean(correct))
```

## Exercises

How can we: 

 * View the entire data set? (*View*)
 
 * Calculate the by-subject averages for all three letter sizes? (*group_by*, *summarise*)
 
 * Calculate the by-subject standard deviations around those averages? (*group_by*, *summarise*)
 
 * Attach this information (the averages and SDs) to each row of the present data.frame? (*group_by*, *mutate*)
 
 * Determine whether RTs were on average faster for correct, as compared to incorrect, trials?
 
 * Add a column for log-transformed RTs to the data set?
 
 * Remove the old column for raw RTs? (*select*)
 
 * Sort the data by log-transformed reaction times? (*arrange*)

Say we further have an additional data frame with information about our subjects:

```{r joining, echo=FALSE}
# Let's make some fake subject data.
d.subj = d %>%
  select(subject) %>%
  distinct() %>%
  rowwise() %>%
  mutate(
    gender = factor(ifelse(rbinom(1,1,.5) == 1, "female", "male")),
    age = round(rnorm(1, 20, 1), 0)
  )

print(d.subj)
```  

 * How can we join the information from the two data sources together? (*left_join*)


```{r}
d.subj
d %<>%
  left_join(d.subj)

d
```




# Data visualization

The two main libraries in R we will be using for visualization are *ggplot2* and *plotly*. Ggplot2 provides a grammar of graphics approach to plotting. Plotly let's us interact with our data. In particular, *ggplotly()* wrapped around a ggplot2 figure let's us interact with that figure.

## Ggplot2's components (aesthetic mappings)

In order to plot in ggplot2, we need to understand the way it thinks about visualization. There are excellent online course that explain all of this, so I focus on the basics. 

At the heart of a plot is a mapping between properties of your data (i.e., column in your data frame) and abstract properties of the plot (such as x- or y-coordinates, color, fill, transparency (alpha), linetype, shape, or label information). If we call the function ggplot() in order to create a figure, we specify two arguments: the name of the data frame we want to work with, and the mapping. The latter is done through a helpful function called *aes()*---for aesthetics:

```{r, echo=T, fig.height=3}
ggplot(
  data = d,
  mapping = 
    aes(
      x = size,
      y = RT))

# or equivalently and shorter:
# ggplot(
#   d,
#   aes(
#       x = size,
#       y = RT))
```

### Adding geometric components (geoms)

Notice that this by itself only returns an empty plot. That's the case because we have not yet specified how we want the abstract properties of the graph to be expressed visually. That's achieved by specifying *geom*s (for geometrics), such as points (*geom_point*), lines (*geom_line*), histograms (*geom_histogram*), lineranges (*geom_linerange*), and many similar functions (I take it you're getting th hang for the naming scheme ...). You can find all of them on the [ggplot2 cheatsheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf). 

We add such components to a plot with "+". 

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d,
  mapping = 
    aes(
      x = size,
      y = RT)) +
  geom_point()

plot(p)
```
Notice that this plot is not particularly helpful! There is a lot of points and they are all overlaid, making it hard to get a clear idea of the data. One way we can improve this plot is to add transparency to the points. Another way is to jitter the points along the x-axis. Both approaches help convey information about the distribution of RTs at each letter size, and sometimes---especially, when we have a lot of data---we might want to combine both approaches. For example, here's the same plot with just transparency added. Since we have quite a bit of data, it still does not convey much information:


```{r, echo=T, fig.height=3}
p = ggplot(
  data = d,
  mapping = 
    aes(
      x = size,
      y = RT)) +
  geom_point(alpha = .1)

plot(p)
```

And here is the plot with some additional jitter added along the x-axis. Note that we intentionally avoid jitter along the y-axis:

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d,
  mapping = 
    aes(
      x = size,
      y = RT)) +
  geom_point(alpha = .1, position = position_jitter(height = 0))

plot(p)
```
That's much better already, giving us a much clearer idea of how the data is distributed! We are now in a position to think about how to include information about condition in this plot. There are a few ways to do this. For example, we could use different point shapes or colors for the easy and hard condition. Here, we do the latter:

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d,
  mapping = 
    aes(
      x = size,
      y = RT,
      color = condition)) +
  geom_point(alpha = .05, position = position_jitter(height = 0))

plot(p)
```
### Visualizing data summaries

We could also summarize the data and plot a bootstrapped 95% confidence interval as a pointrange. In this case, we're specifying a statistical summary of the data and, as part of that, specify through which type of geom we would like it to be expressed:

```{r}
p +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", shape = 15, size = 1)
```

Alternatively, we could add a violin (essentially a mirrored density distribution):

```{r, fig.height=3}
p +
  geom_violin(fill = NA)
```

### Scales and coordinate systems

Sometimes we don't want to see all of the data, or we want to zoom into some ranges of our data. We can do so by explicitly specifying the x- and y-limits of our coordinate system:


```{r, fig.height=3}
p +
  geom_violin(fill = NA) +
  coord_cartesian(ylim = c(200,1500))
```

Note that this zooms into parts of our data without excluding any data (e.g., from the calculation of the violins, which have the same shape as above). If we want to exclude data, transform data or in other ways change the way the aesthetical mappings are interpreted, this is achieved through *scales*. For example, the following *ex*cludes all RTs below 300 and above 2000. Note how that changes the violin plots (as it should: they estimate the ditribution of RTs):

```{r, fig.height=3}
p +
  geom_violin(fill = NA) +
  scale_y_continuous(limits = c(200,1500))
```

Since reaction times often have distributions that are more lognormal, rather than normal, let's update our original plot to use a log-transformed y-axis:

```{r, fig.height=3}
p = p +
  geom_violin(fill = NA) +
  scale_y_log10()

plot(p)
```

### Facets

If we want to have separate panels conditional on another variable, we can do so through *facet*s. There are two major facet functions, *facet_wrap* (to have panels conditional on one variable) and *facet_grid* (conditional on two variables). For example, we can have separate panels for each subject:

```{r, fig.height=12, fig.width=8}
p +
  geom_violin(fill = NA) +
  facet_wrap(facets = ~ subject)
```

Or we could show by-subject RTs in two columns, separately for false and correct answers. Here, we do so after first sampling 6 random subjects (since the plot would otherwise be rather large).

```{r, fig.height=8, fig.width=4}
# We can update a plot with new data using %+%
p %+%
  (d %>% filter(subject %in% sample(levels(subject), 6))) +
  geom_violin(fill = NA) +
  facet_grid(facets = subject ~ correct, labeller = label_both)
```

## Pipes (again)

Of course, we can use pipes to pipe the data frame into the plotting function, optionally after first piping the data through some additional *dplyr* operations (since the output of that entire pipe is again a data frame):

```{r, echo = T, fig.height=3}
d %>%
  filter(size != "1") %>%
  ggplot(
    aes(
      x = size,
      y = RT,
      color = condition)) +
  geom_point(alpha = .05, position = position_jitter()) 
```

## Exercises

 * Plot a histogram of the RTs by letter size. Make on version where you plots the histograms in different facets, and another version where you have only one facet and use fill color to distinguish between letter sizes. (*geom_histogram*)
 * Plot the average proportion of correct answers by letter size as a pointrange.
 * Do the same, but first average by subject and letter size, and then plot the average (and confidence interval) of those by-subject averages of correct responses.
 * Try to make a pie chart that shows the proportion correct for the three letter sizes. (*coord_polar*)

# TBA

 * making x-axis have right units
 * axis labels
 * panel grid, etc.

# Session info
```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
