---
title: 'Data Wrangling and Visualization 101'
author: "for BCS 206"
date: "Fall 2020"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, results = "markup", warning = FALSE, cache = TRUE,
  fig.align = "center", fig.width = 6.5)
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("R.matlab")   # for interacting with MatLab
library("readxl")     # for interacting with Excel
library("cowplot")    # for combining figures
library("plotly")     # for interactive plots

theme_set(theme_bw())
```

# Goals for next two weeks

Over the next two weeks, we will introduce you to the basics of data visualization in R (Jaeger group; taught by Florian) and MatLab (Haefner and Mitchell groups; taught by Sabya). The topics we aim to cover are the following:

 * Data wrangling: Turning the data into the form you need (*dplyr*)
 * Data visualization:
   * General principles
   * How to plot in R (*ggplot*, *plotly*)
 * Documenting your code:
   * R Markdown / MatLab notebook
 * Introduction to confidence intervals
 
We only have a relatively short time, so we will focus on learning what tools are available and on *examples* of use (rather than an in-depth tutorial). There are great online tutorials and cheat sheets that contain further information. Specifically:

 * **By Friday 10/30:**
   * Send your initial data (e.g., excel, matlab, or csv file) to both Sabya and Florian (you can use Slack). If you won’t have your own data by then (which might well happen), that’s ok. In that case, please ask your PI whether they have an old data set with data similar to the one that you would be analyzing. 

 * Monday 11/2:
   * **Prepare before class:** You will receive a data set ahead of class (described below). Load it into Matlab/R and familiarize yourself with its structure. Quick primers are available online for both R (https://rstudio.cloud/learn/primers/1) and Matlab (**SABYA**), as are tutorials on how to load data. Go through them *before* class.
   * In class, we will use that data to illustrate how we can visualize our data at various levels of summarization. 
<!--  
 + Whole class together: briefly introduce data
  + DV = continuous or discretized continuous outcome
  + IVs:
    + 1 predictor that is underlyingly continuous but only discrete steps have been sampled from during experiment
    + 1 predictor that is two-way categorical condition
 + Split into R/Matlab groups: 
  + scatter plot: 
  + start with simple scatter plot, ignoring condition.
  + add color
  + deal with point overlap (jitter, transparency)
  + add data summary (violin, boxplot)
  + increase summarization (point range with confidence interval [do not explain yet how CI is derived])
 + Rejoin at about 10am into whole class to discuss pros and cons of the different levels of summary.
//-->
     

 * Wednesday 11/4: 
   * **Prepare before class:** Load your group's data. For at least one subject in your data, try to repeat the different plots we've introduced on Monday for your own data. You will be asked to present your efforts in class (to go through your script while sharing your screen). It's ok to get stuck, but please use Slack to ask for help prior to class. 
   * In class, we will also go through problems/errors you might have encountered while trying to create visualizations of your data. 
<!--  
 + Split into R/Matlab groups: 
  + debug and go through problems with students. (have them present and then explain/help) 
  + in particular, there might be issues with getting the data into the right format or basic R/Matlab questions.
  + challenges could also come up because there's data from multiple subjects.
  + SABYA: we could also try to cover legends etc. here if time permits. Then we would use the final session (a week later) to talk about what to do with repeated measures data (why we're summarizing the data down to the level of one data point per subject per condition when plotting CIs for repeated measures data, such as the "average subject" for your group.)
 + Rejoin at about 10am into whole class to discuss what's working with the figures and what's still missing.
//-->
 * Monday 11/9:
   * Summarizing variability in your data---and thus the researcher's *uncertainty* about the central tendencies in the data
   * Introduction to standard deviations, standard errors, and confidence intervals
 * Wednesday 11/11:
   * Preparing your visualizations for presentation (captions, axis titles, legends, and other annotations)
   * Saving your visualizations (format, dimensions)
 
# Preliminaries

## Version control

RStudio makes version control, data backup, and data sharing easy (e.g., via Github.com). To use it, download and install git on your computer. Get a free github.com or bitbucket.com account. You only have to do this once.

Then, for each project, create a new project in RStudio and link it to the remote repository (select “Create project” > “Version control”). You will have to enter a URL for the remote repository, which you get, for example, at github.com under the repository’s main page by clicking the “Clone or download button”.

For step by step instructions, see:

 * [Setting up RStudio for version control](http://www.geo.uzh.ch/microsite/reproducible_research/post/rr-rstudio-git/)
 * [RStudio help on version control](https://support.rstudio.com/hc/en-us/articles/200532077?version=1.1.442&mode=desktop)
 * [Reverting a file to an earlier version](https://stackoverflow.com/questions/38465025/how-do-i-revert-to-a-previous-git-in-r-studio)


## Reproducibility and literate coding

R and RStudio support reproducibility oriented literate coding via Sweave and Knitr: lab books, presentations, and papers can weave/knit together data, code, and text. The document you share contains the code needed to create its outputs (figures, tables, etc.). This is achieved by combining latex or R markdown with R code (or, for that matter, code from other programming languages). For an excellent video-based introduction, see this [tutorial on R markdown](https://rmarkdown.rstudio.com/lesson-1.html). *This document is R markdown compiled with RStudio's knitr.



# Data wrangling

The *R* libraries *dplyr* provide us with efficient ways to transform ('wrangle') our data tables. The library *magrittr* let's us concatenate these operations in transparent and easy to read code. 

## An example data set

```{r, include=FALSE}
# Let's make some fake data
d = 
  crossing(
    condition = c("A","B","C"),
    trial = 1:64,
    subject = 1:42
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "A" ~ .5,
      condition == "B" ~ .61,
      condition == "C" ~ .88
    )),
    muLogRT = case_when(
      condition == "A" ~ 6.2,
      condition == "B" ~ 6.2,
      condition == "C" ~ 7.3
    )
  ) %>%
  group_by(subject) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, 1.5),
    muLogRT.bySubject = rnorm(1, 0, 0.3)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(exp(rnorm(1, muLogRT + muLogRT.bySubject - .2 * correct, .05)), 3)
  ) %>%
  as_tibble() %>%
  select(-starts_with("mu")) %>%
  mutate_at(c("condition", "subject"), factor)
```

We will illustrate the use of *dplyr* with the following data from an experiment with a 2AFC task in three within-subject conditions (A, B, C), for which we have extracted correctness (1 = correct; 0 = incorrect) and reaction times (RT):

```{r, echo=TRUE}
summary(d)

glimpse(d)
```


## Dplyr's verbs
Dplyr has 'verbs' like filter, select, summarize, mutate, transmute, etc. to let use conduct operations on our data, and reshape the data frame into the format we need. We can use dplyr, for example, to calculate the proportion correct answers in our experiment by using *summarise*. 

```{r, echo=T}
summarise(d, meanCorrect = mean(correct))
```

Or just for condition A:

```{r, echo=T}
d.A = filter(d, condition == "A")
summarise(d.A, meanCorrect = mean(correct))
```

## Maggritr's pipes

Here we will use only of the 'pipes' magrittr provides:

 * x %>% f: takes x and hands it to the function f on the right, as f's first argument
 * x %<>% f1 %>% f2 %>% etc.: takes x hands it to f1, takes the output of f1 and hands it to f2, etc. And since the first pipe was %<>% (rather than just %>%), the final result will be written back into x.

![Magritt's pipe](./figures/otherpipe.jpeg){width=150px}

![Magrittr's pipe](./figures/pipe.png){width=150px}

## Putting it together: Wrangling through pipes

Remember how we got the mean proportion correct for just Condition A?
```{r, echo=T}
d.A = filter(d, condition == "A")
summarise(d.A, meanCorrect = mean(correct))
```

This is inelegant and hard to read. Pipes let us make this more transparent:

```{r, echo=T}
d %>%
  filter(condition == "A") %>%
  summarise(meanCorrect = mean(correct))
```

And this advantage becomes even clearer, the more operationgs we concatenate. For example, *group_by* is an elegant operator that tells the pipes to conduct all subsequent operations for each of the groups (and then put all the separate outcomes back together into a single data frame). So if we want the proportion correct for all groups:

```{r, echo=T}
d %>%
  group_by(condition) %>%
  summarise(meanCorrect = mean(correct))
```

## Exercises

How can we: 

 * View the entire data set? (*View*)
 
 * Calculate the by-subject averages for all three conditions? (*group_by*, *summarise*)
 
 * Calculate the by-subject standard deviations around those averages? (*group_by*, *summarise*)
 
 * Attach this information (the averages and SDs) to each row of the present data.frame? (*group_by*, *mutate*)
 
 * Determine whether RTs were on average faster for correct, as compared to incorrect, trials?
 
 * Add a column for log-transformed RTs to the data set?
 
 * Remove the old column for raw RTs? (*select*)
 
 * Sort the data by log-transformed reaction times? (*arrange*)

Say we further have an additional data frame with information about our subjects:

```{r joining, echo=FALSE}
# Let's make some fake subject data.
d.subj = d %>%
  select(subject) %>%
  distinct() %>%
  rowwise() %>%
  mutate(
    gender = factor(ifelse(rbinom(1,1,.5) == 1, "female", "male")),
    age = round(rnorm(1, 20, 1), 0)
  )

print(d.subj)
```  

 * How can we join the information from the two data sources together? (*left_join*)


```{r}
d.subj
d %<>%
  left_join(d.subj)

d

```




# Data visualization

The two main libraries in R we will be using for visualization are *ggplot2* and *plotly*. Ggplot2 provides a grammar of graphics approach to plotting. Plotly let's us interact with our data. In particular, *ggplotly()* wrapped around a ggplot2 figure let's us interact with that figure.

## Ggplot2's components (aesthetic mappings)

In order to plot in ggplot2, we need to understand the way it thinks about visualization. There are excellent online course that explain all of this, so I focus on the basics. 

At the heart of a plot is a mapping between properties of your data (i.e., column in your data frame) and abstract properties of the plot (such as x- or y-coordinates, color, fill, transparency (alpha), linetype, shape, or label information). If we call the function ggplot() in order to create a figure, we specify two arguments: the name of the data frame we want to work with, and the mapping. The latter is done through a helpful function called *aes()*---for aesthetics:

```{r, echo=T, fig.height=3}
ggplot(
  data = d,
  mapping = 
    aes(
      x = condition,
      y = RT))

# or equivalently and shorter:
# ggplot(
#   d,
#   aes(
#       x = condition,
#       y = RT))
```

### Adding geometric components (geoms)

Notice that this by itself only returns an empty plot. That's the case because we have not yet specified how we want the abstract properties of the graph to be expressed visually. That's achieved by specifying *geom*s (for geometrics), such as points (*geom_point*), lines (*geom_line*), histograms (*geom_histogram*), lineranges (*geom_linerange*), and many similar functions (I take it you're getting th hang for the naming scheme ...). You can find all of them on the [ggplot2 cheatsheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf). 

We add such components to a plot with "+". We can also further explicitly specify any unused aesthetical properties of any geom. For example, to plot all the RTs for all three conditions with some transparency so that we see whether points cluster, plus some jittering along the x-axis (for the same reason):

```{r, echo=T, fig.height=3}
p = ggplot(
  data = d,
  mapping = 
    aes(
      x = condition,
      y = RT)) +
  geom_point(alpha = .1, position = position_jitter())

plot(p)
```


We could also summarize the data and plot a bootstrapped 95% confidence interval as a pointrange. In this case, we're specifying a statistical summary of the data and, as part of that, specify through which type of geom we would like it to be expressed:

```{r}
p +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", color = "blue")
```

Alternatively, we could add a violin plot (essentially a mirrored density distribution, in this case displayed vertically on top of the points):

```{r, fig.height=3}
p +
  geom_violin(fill = NA, color = "blue")
```

### Scales and coordinate systems

Sometimes we don't want to see all of the data, or we want to zoom into some ranges of our data. We can do so by explicitly specifying the x- and y-limits of our coordinate system:


```{r, fig.height=3}
p +
  geom_violin(fill = NA, color = "blue") +
  coord_cartesian(ylim = c(300,2000))
```

Note that this zooms into parts of our data without excluding any data (e.g., from the calculation of the violins, which have the same shape as above). If we want to exclude data, transform data or in other ways change the way the aesthetical mappings are interpreted, this is achieved through *scales*. For example, the following *ex*cludes all RTs below 300 and above 2000. Note how that changes the violin plots (as it should: they estimate the ditribution of RTs):

```{r, fig.height=3}
p +
  geom_violin(fill = NA, color = "blue") +
  scale_y_continuous(limits = c(300,2000))
```

Since reaction times often have distributions that are more lognormal, rather than normal, let's update our original plot to use a log-transformed y-axis:

```{r, fig.height=3}
p = p +
  geom_violin(fill = NA, color = "blue") +
  scale_y_log10()

plot(p)
```

### Facets

If we want to have separate panels conditional on another variable, we can do so through *facet*s. There are two major facet functions, *facet_wrap* (to have panels conditional on one variable) and *facet_grid* (conditional on two variables). For example, we can have separate panels for each subject:

```{r, fig.height=12}
p +
  geom_violin(fill = NA, color = "blue") +
  facet_wrap(facets = ~ subject)
```

Or we could show by-subject RTs in two columns, separately for false and correct answers. Here, we do so after first sampling 6 random subjects (since the plot would otherwise be rather large).

```{r, fig.height=8}
# We can update a plot with new data using %+%
p %+%
  (d %>% filter(subject %in% sample(levels(subject), 6))) +
  geom_violin(fill = NA, color = "blue") +
  facet_grid(facets = subject ~ correct, labeller = label_both)
```

## Pipes (again)

Of course, we can use pipes to pipe the data frame into the plotting function, optionally after first piping the data through some additional *dplyr* operations (since the output of that entire pipe is again a data frame):

```{r, echo = T, fig.height=3}
d %>%
  filter(condition != "A") %>%
  ggplot(
    aes(
      x = condition,
      y = RT)) +
  geom_point(alpha = .1, position = position_jitter()) 
```

## Exercises

 * Plot a histogram of the RTs by condition. Make on version where you plots the histograms in different facets, and another version where you have only one facet and use fill color to distinguish between conditions. (*geom_histogram*)
 * Plot the average proportion of correct answers by condition as a pointrange.
 * Do the same, but first average by subject and condition, and then plot the average (and confidence interval) of those by-subject averages of correct responses.
 * Try to make a pie chart that shows the proportion correct for the three conditions. (*coord_polar*)



# Session info
```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
