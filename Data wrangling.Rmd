---
title: 'Data Wrangling 101'
author: "for BCS 206"
date: "Fall 2019"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, results = "markup", warning = FALSE,
  fig.align = "center", fig.width = 6.5)
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("R.matlab")   # for interacting with MatLab
library("cowplot")    # for combining figures
library("plotly")     # for interactive plots

theme_set(theme_bw())
```

# Preliminaries

## Version control

RStudio makes version control, data backup, and data sharing easy (e.g., via Github.com). To use it, download and install git on your computer. Get a free github.com or bitbucket.com account. You only have to do this once.

Then, for each project, create a new project in RStudio and link it to the remote repository (select “Create project” > “Version control”). You will have to enter a URL for the remote repository, which you get, for example, at github.com under the repository’s main page by clicking the “Clone or download button”.

For step by step instructions, see:

 * [Setting up RStudio for version control](http://www.geo.uzh.ch/microsite/reproducible_research/post/rr-rstudio-git/)
 * [RStudio help on version control](https://support.rstudio.com/hc/en-us/articles/200532077?version=1.1.442&mode=desktop)
 * [Reverting a file to an earlier version](https://stackoverflow.com/questions/38465025/how-do-i-revert-to-a-previous-git-in-r-studio)


## Reproducibility and literate coding

R and RStudio support reproducibility oriented literate coding via Sweave and Knitr: lab books, presentations, and papers can weave/knit together data, code, and text. The document you share contains the code needed to create its outputs (figures, tables, etc.). This is achieved by combining latex or R markdown with R code (or, for that matter, code from other programming languages). For an excellent video-based introduction, see this [tutorial on R markdown](https://rmarkdown.rstudio.com/lesson-1.html). *This document is R markdown compiled with RStudio's knitr.



# Data wrangling

The *R* libraries *dplyr* provide us with efficient ways to transform ('wrangle') our data tables. The library *magrittr* let's us concatenate these operations in transparent and easy to read code. 

## An example data set

```{r, include=FALSE}
# Let's make some fake data
d = 
  crossing(
    condition = c("A","B","C"),
    trial = 1:64,
    subject = 1:42
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "A" ~ .5,
      condition == "B" ~ .61,
      condition == "C" ~ .88
    )),
    muLogRT = case_when(
      condition == "A" ~ 6.2,
      condition == "B" ~ 6.2,
      condition == "C" ~ 7.3
    )
  ) %>%
  group_by(subject) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, 1.5),
    muLogRT.bySubject = rnorm(1, 0, 0.3)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(exp(rnorm(1, muLogRT + muLogRT.bySubject - .2 * correct, .05)), 3)
  ) %>%
  as_tibble() %>%
  select(-starts_with("mu")) %>%
  mutate_at(c("condition", "subject"), factor)
```

We will illustrate the use of *dplyr* with the following data from an experiment with a 2AFC task in three within-subject conditions (A, B, C), for which we have extracted correctness (1 = correct; 0 = incorrect) and reaction times (RT):

```{r, echo=TRUE}
summary(d)

glimpse(d)
```


## Dplyr's verbs
Dplyr has 'verbs' like filter, select, summarize, mutate, transmute, etc. to let use conduct operations on our data, and reshape the data frame into the format we need. We can use dplyr, for example, to calculate the proportion correct answers in our experiment by using *summarise*. 

```{r, echo=T}
summarise(d, meanCorrect = mean(correct))
```

Or just for condition A:

```{r, echo=T}
d.A = filter(d, condition == "A")
summarise(d.A, meanCorrect = mean(correct))
```

## Maggritr's pipes

Here we will use only of the 'pipes' magrittr provides:

 * x %>% f: takes x and hands it to the function f on the right, as f's first argument
 * x %<>% f1 %>% f2 %>% etc.: takes x hands it to f1, takes the output of f1 and hands it to f2, etc. And since the first pipe was %<>% (rather than just %>%), the final result will be written back into x.

![Magritt's pipe](./figures/otherpipe.jpeg){width=150px}

![Magrittr's pipe](./figures/pipe.png){width=150px}

## Putting it together: Wrangling through pipes

Remember how we got the mean proportion correct for just Condition A?
```{r, echo=T}
d.A = filter(d, condition == "A")
summarise(d.A, meanCorrect = mean(correct))
```

This is inelegant and hard to read. Pipes let us make this more transparent:

```{r, echo=T}
d %>%
  filter(condition == "A") %>%
  summarise(meanCorrect = mean(correct))
```

And this advantage becomes even clearer, the more operationgs we concatenate. For example, *group_by* is an elegant operator that tells the pipes to conduct all subsequent operations for each of the groups (and then put all the separate outcomes back together into a single data frame). So if we want the proportion correct for all groups:

```{r, echo=T}
d %>%
  group_by(condition) %>%
  summarise(meanCorrect = mean(correct))
```

# Exercises

How can we: 

 * View the entire data set? (*View*)
 
 * Calculate the by-subject averages for all three conditions? (*group_by*, *summarise*)
 
 * Calculate the by-subject standard deviations around those averages? (*group_by*, *summarise*)
 
 * Attach this information (the averages and SDs) to each row of the present data.frame? (*group_by*, *mutate*)
 
 * Determine whether RTs were on average faster for correct, as compared to incorrect, trials?
 
 * Add a column for log-transformed RTs to the data set?
 
 * Remove the old column for raw RTs? (*select*)
 
 * Sort the data by log-transformed reaction times? (*arrange*)

Say we further have an additional data frame with information about our subjects:

```{r joining, echo=FALSE}
# Let's make some fake subject data.
d.subj = d %>%
  select(subject) %>%
  distinct() %>%
  rowwise() %>%
  mutate(
    gender = factor(ifelse(rbinom(1,1,.5) == 1, "female", "male")),
    age = round(rnorm(1, 20, 1), 0)
  )

print(d.subj)
```  

 * How can we join the information from the two data sources together? (*left_join*)







# Combining data wrangling and visualization: an example from the Haefner group

This group seeks to replicate [Herce Castañón et al. (2019)](https://www.nature.com/articles/s41467-019-09330-7).

![Figure 2 from Herce Castañón et al. (2019)](./figures/HerceCastanonFigure2.png)

## Design

The design of the present study crossed two levels of contrast (Low = 15%, High = 60%), 3 levels of variance (0, 4, 10), and how the trials in the block were cued (L = left, R = right, N = uncued), for a total of 2 x 3 x 3 = 18 within-subject conditions.

## Loading data

The data are stored in a MatLab (.mat) file. The file contains one matrix with fields: participant, exp(eriment), stimuli and response. Within each field, there is further information. The important information seems to be in the response field. Some of the important parts include:

 * responseRight: the response of the subject (0 for CCW, 1 for CW, w.r.t horizontal)
 * correct: what the correct answer is (0 for CCW, 1 for CW, w.r.t horizontal)
 * accuracy: whether subject got the correct answer (1) or not (0)
 * reaction time: time in seconds the subject took to answer
 * confidence: whether the subject was confident in their answer (1) or not (-1)
 * cue: whether the cue on that trials is left (-1), right (1), or no cue (0)
 * contrast: the contrast of the gabor patch on that trial
 * variance: variability in the orientation of gratings of gabor patches on that trial
 * isCuedBlock: whether a block (of trials) will have cues (1) or no cues (0)

```{r load data, results="hide", echo=T}
# Load a matlab file and extract the "data" matrix out of it
d.haefner = readMat("./data/Haefner/uncertaintyV1-subject18-1-EarlyQuit.mat")
d.haefner = d.haefner[["data"]][,,1][["response"]][,,1]
d.haefner[["trueOrientaions"]] <- NULL

# Look at what we've imported. 
# NB: str() gives your the structure of an R object
str(d.haefner)

d.haefner %<>%
  map(.f = function(x) c(x)) %>%
  as_tibble() 

# The data we have are preliminary pilot data from one of the 
# experimenters, and that run did contain all trials. We omit 
# all the trials with missing information.
d.haefner %<>%
  na.omit()

# Add the definition of the three conditions of interest in the
# original paper
d.haefner %<>%
  mutate(
    condition = case_when(
        variance == min(variance) & contrast == max(contrast) ~ "baseline",
        variance == max(variance) & contrast == max(contrast) ~ "high variance",
        variance == min(variance) & contrast == min(contrast) ~ "low contrast",
        T ~ ""
    )
  )
```


Now that we've imported the data into an R data frame (or *tibble*), let's have a look at it. First, we can get a general idea of the data by using str() (for structure) or print():

```{r}
# NB: if you call an R object without any function, R
#     automatically applies the print() function to the
#     object. For a tibble/data.frame this provides an
#     overview.
d.haefner
# same as print(d.haefner)

# To view the entire data, use View(d.haefner)
# To get a glimpse, use glimpse(d.haefner)
```

To instead get a summary of the data:

```{r}
summary(d.haefner)
```

## Figure 2 from Herce Castañón et al.


### Panel A

We begin by plotting the proportion of correct choices for *all* conditions:

```{r}
p = position_dodge(.9)

p1 = d.haefner %>%
  ggplot(
    aes(x = factor(contrast),
        y = accuracy,
        fill = factor(variance))
  ) +
  stat_summary(fun.y = mean, geom="bar", position=p) +
  stat_summary(fun.data = mean_cl_boot, geom = "linerange", position=p) +
  facet_wrap(~ cue) +
  scale_x_discrete(
    "Contrast",
    breaks = c(.15, .6),
    labels = c("low", "high")
  ) +
  scale_y_continuous("Proportion correct") +
  scale_fill_discrete("Variance")

plot(p1)
```

### Panel B

We begin by plotting the proportion of CW choices for *all* conditions:


```{r, warning=FALSE}
p2 = d.haefner %>%
  ggplot(
    aes(x = orientationMean,
        y = responseRight,
        color = factor(cue))
  ) +
  stat_summary(
    data = d.haefner %>%
      mutate(orientationBin = ntile(orientationMean, 6)) %>%
      group_by(cue, contrast, variance, orientationBin) %>%
      summarise_at(
        c("orientationMean", "responseRight"),
        mean),
  # could be changed to pointrange and mean_cl_boot, once there are
  # several subjects
    fun.y = mean, geom = "point") + 
  geom_smooth(method = "glm", 
              se = FALSE, # remove to see CIs
              method.args = list(family = binomial)) +
  facet_grid(contrast ~ variance, 
             labeller = 
               labeller(.rows = label_both, .cols = label_both)) +
  scale_x_continuous("mean orientation\n(relative to horizontal)") +
  scale_y_continuous("Proportion of\nCW choices") +
  scale_color_manual(
    "Cue",
    breaks = c(-1,0,1),
    labels = c("left", "none", "right"),
    values = c("red", "black", "blue")
  ) 

p2
```


```{r, include=FALSE}
# if one wanted to share the facets of interest
p2 +
  geom_rect(
    data = d.haefner %>% 
      group_by(contrast, variance, condition) %>%
      summarise(),
    inherit.aes = F,
    aes(fill = condition),
    xmin = -Inf, xmax = Inf,
    ymin = -Inf, ymax = Inf, alpha = 0.2) +
  scale_fill_manual(
    "Condition",
    breaks = c("", "baseline", "high variance", "low contrast"),
    values = c(NA, "green", "yellow", "orange")
  )
```

### Panel A and B together

```{r, fig.width=10, fig.height=5}
p1 = d.haefner %>%
  filter(condition != "", cue == 0) %>%
  mutate(condition = factor(condition, 
                            levels = c("baseline", "low contrast", "high variance"))) %>%
  ggplot(
    aes(x = condition,
        y = accuracy)
  ) +
  stat_summary(fun.y = mean, geom="bar", position=p, fill = "blue", alpha = .5, color = NA) +
  stat_summary(fun.data = mean_cl_boot, geom = "linerange", position=p) +
  scale_x_discrete("") +
  scale_y_continuous("Proportion correct") +
  coord_cartesian(ylim = c(0,1)) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

p2 = d.haefner %>%
  filter(condition != "") %>%
  mutate(condition = factor(condition, levels = c("baseline", "low contrast", "high variance"))) %>%
  ggplot(
    aes(x = orientationMean,
        y = responseRight,
        color = factor(cue))
  ) +
  stat_summary(
    data = d.haefner %>%
      filter(condition != "") %>%
      mutate(orientationBin = ntile(orientationMean, 6)) %>%
      group_by(cue, condition, orientationBin) %>%
      summarise_at(
        c("orientationMean", "responseRight"),
        mean),
    fun.y = mean, geom = "point") + 
  geom_smooth(method = "glm", 
              se = FALSE, # remove to see CIs
              method.args = list(family = binomial)) +
  facet_wrap(~ condition) +
  scale_x_continuous("mean orientation (relative to horizontal)") +
  scale_y_continuous("Proportion of CW choices") +
  scale_color_manual(
    "Cue",
    breaks = c(-1,0,1),
    labels = c("left", "none", "right"),
    values = c("red", "black", "blue")
  ) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "top"
  )

plot_grid(
  plotlist = list(p1, p2),
  nrow = 1,
  rel_widths = c(.30, .70),
  labels = c("a", "b"),
  align = "hv",
  axis = "bt"
)
```

# Session info
```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
