---
title: "Sampling based approaches"
author: "T. Florian Jaeger"
date: "2/26/2020"
output: 
  bookdown::gitbook:
   lib_dir: assets
   split_by: none
   config:
    toolbar:
      position: static
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  results = "markup", warning = FALSE, cache = TRUE,
  fig.align = "center", fig.width = 6.5)
```

```{r constants, include = F}
n.samples = 2000
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("boot")       # for automatic bootstrap
```

# This document
This document is created in R markdown. R markdown combines text with R code, allowing us to see the code and its output, embedded within the text describing the code. If you have the original R markdown file (file extension .Rmd), you can 'knit' the document into an HMTL, PDF, or Word file. Just be aware that the code executes a good number of sampling-based simulations, and these simulations take some time to complete. The parameter that determines how many samples are used in each of the sampling simulations is `n.samples`. You can change the value of `n.samples` to a small number (e.g., `10`) to speed up compilation.

# Recap of some terminology
In BCS, the goal of conducting experiments is to advance our understanding of the brain/mind. We aim to test the **predictions** of **theories** or more specific **hypotheses**. These theories typically constitute propositions about causes, and the statistical tests we conduct typically assess the predicted correlations that would result from the hypothesized causal relations. For example, we might hypothesize that our visual systems integrate incoming perceptual inputs with prior expectations. This leads us to predict---skipping over many details here---that more expected (e.g., more frequent) object should be recognized more quickly.

We thus design an experiment, in which we elicit reaction times for visual objects that either occur often in our visual experience (high frequency) or rarely (low frequency). In this example, reaction times are our **dependent variable** or **outcome**, and frequency is an **independent variable** or **predictor** (here we operationalize frequency as a binary predictor: high vs. low frequency). Often we talk about **conditions** of experiments. These conditions are the independent / predictor variables in our analyses. The dependent / outcome variables are derived from the **measures** we collect.

When we analyze data from our experiment, we assume that the reaction time **observations** we obtain constitute a **sample** of the underlying **population** of reaction times. We then use this finite sample to draw **inferences** about the population. For example, a really typical question we ask would be whether the means of the population differ between two conditions. Sticking with our example, we might test our hypothesis about the brain/mind by asking whether we can confidently conclude that reaction times are indeed smaller in for high frequency objects, compared to reactions times for high frequency objects. Either way, this constitutes an inference we're aiming to draw about the world, based on the finite sample of observations we collected in our experiment.

When we ask such a question---e.g., whether reaction times are indeed smaller in for high frequency objects---we typically ask them in the form of "Are reactions times for high frequency objects smaller *on average*?". We thus aim to draw inferences about a statistic (the mean) of the underlying population. This in turn is typically done by looking at the same statistic (the mean) in the *sample*, along with some measure of the **uncertainty** we have about that sample statistic. We have this uncertainty because we believe that our observations constitute measures taken from a **random variable**, so that our observations will be ... well, variable.

## Sampling-based approaches

We can distinguish between two approaches to the statistical inferences scientists try to draw about the world from the samples they obtain in their experiments. For example, how do we quantify the mean difference in reaction time between the high vs. low frequency condition of our example scenario? One approach is analytic, using mathematical knowledge about the distributions of our variables. For example, the mean of the difference between two normally distributed (non-correlated) variables is the difference between the means of the two distributions; the standard deviation of the difference is the square root of the sum of the variances of the two distributions. So if we assume that reaction times (RTs) are normally distributed, and if high frequency objects would lead to an RT distribution with mean 250 and standard deviation 21, and low frequency objects would lead to an RT distribution with mean 293 and standard deviation 32, then the difference between the two RT distributions would be distributed with mean 250 - 293 = ```r 250 - 293``` and standard deviation sqrt(21^2 + 32^2) = ```r sqrt(21^2 + 32^2)```.

The second approach is the **sampling-based approach**. For this approach, we repeatedly draw samples from the two distributions and calculate the difference between each pair of samples (this is not exactly the same use of the word sample as in the previous section where we referred to samples of observations we collect in experiments; but the two uses are related). These differences then form a distribution, and we can get the mean and standard deviation of that distribution. More generally, we can obtain any quantity from the distribution---even more complex ones---by repeatedly sampling from it. As we'll show below, this lets us, for example, construct confidence intervals, or it lets us assess how likely we would be by chance to see the differences we observe in our sample (and thus how informative these differences are). The sampling-based approach is particularly powerful when we do not the analytic solution (or are not sure about it). 

To make this more concrete, let's go through an example. 

# An example problem

```{r, include = F}
# We will generate some fake data to work with, but you could plug your own real data in here 
# (and change the code below so that it matches the names of the variables in your data)
set.seed(123456)

n.subject = 48
n.trial = 24

# Let's make some (fake) data
d.orig = 
  crossing(
    subject = 1:n.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "low frequency" ~ .78,
      condition == "high frequency" ~ .91
    )),
    muLogRT = case_when(
      condition == "low frequency" ~ 6.5,
      condition == "high frequency" ~ 6.1
    )
  ) %>%
  group_by(subject, condition) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, .75),
    muLogRT.bySubject = rnorm(1, 0, .4)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(100 + exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition)

d = d.orig %>%
    select(-starts_with("mu"))
```

Let's say we have an experiment with `r n.subject` subjects, in which each subject saw `r n.trial` trials for each of two conditions (for a total of `r n.trial * 2` trials per subject). On each trial, subjects saw a picture of an object and had to decide as quickly and accurately whether the picture depicts something animate (a living thing) or not. The two conditions manipulate whether the object is something that we see frequently in our live ("high frequency") or not ("low frequency"). We collected both reaction times of subjects' responses (`RT`), and whether those responses were accurate (`correct`). We hypothesize that more frequent objects will be responded to more quickly and accurately. 

Here's the top of that data table:

```{r, echo=F}
d
```

And here is a summary. We can see that reaction times vary quite a bit, and that accuracy is (unsurprisingly) relatively high:

```{r, echo=F}
summary(d)
```

To test our hypothesis we want to be able to visualize the mean reaction times (RTs) and accuracy for each condition, and we want to be able to assess whether the differences between the conditions are meaningful. For example, we might want to test whether the differences between conditions are "significant".

## Visualization

The first thing we do is to visualize our data by condition. Before we worry about how to best do this for all of our subjects, let's start by plotting the data for just a single subject. The following plot shows the mean RT and accuracy for subject 1:

```{r, echo=F, fig.show='hold', out.width="50%", fig.align = "default"}
theme_set(theme_bw())

d %>%
  filter(subject == 1) %>%
  ggplot(aes(x = condition, y = RT)) +
  stat_summary(fun = mean, geom = "point", size = 1, color = "blue")
```

These plots show that the RTs and accuracy differ between conditions, but not whether these differences are meaningful. In particular, there is no measure of uncertainty. So let's add some confidence intervals. For example, we could calculate confidence intervals based on the assumption of normality, in which case the 95\% confidence interval is about +/- 1.96-times the standard error of the mean around the mean. Let's also add all the underlying data from subject 1:

```{r, echo=F, fig.show='hold', out.width="50%", fig.align = "default"}
theme_set(theme_bw())

d %>%
  filter(subject == 1) %>%
  ggplot(aes(x = condition, y = RT)) +
  geom_point(alpha = .3) +
  stat_summary(fun.data = mean_cl_normal, geom = "pointrange", size = 1, color = "blue")
```

The difference for RTs looks rather interpretable, suggesting a very large difference between the two conditions in subject 1. As a rule of thumb, since neither of the confidence intervals for the two conditions contains the mean of the other condition, we would expect that a test (that is also based on the assumption of normality) would return a significant difference in RTs between the high vs. low frequency conditions for subject 1. 

## How was the confidence interval in the above figure obtained? 

One way to obtain the confidence interval shown above is *parametrically*. Under the assumption of normality the 95\% confidence interval is +/- 1.96-times the standard error of the mean, and the standard error is the standard deviation divided by the square root of the number of observations *n*, SE = SD / sqrt(n). As the following table shows, the CIs obtained from this formula match those shown in the above figure:

```{r}
d1 = d %>% 
  filter(subject == 1) %>%
  group_by(condition) %>%
  summarise_at(vars(RT), .funs = list("mean" = mean, "sd" = sd)) %>%
  mutate(se = sd / sqrt(n.trial),
         ci.upper = mean + 1.96 * se,
         ci.lower = mean - 1.96 * se)

d1
```

But **how would we go about calculating a 95\% confidence interval around each mean under the assumption of normality if we *didn't* already know that it's 1.96-times the standard error of the mean?** (as is the case for many non-trivial statistical questions) This is where a sampling-based approaches comes in. We could repeatedly sample the same amount of data from a normal distribution with the same mean and standard deviation as observed for subject 1:

```{r, echo = T}
# Let's 
#   1) draw 24 samples of RTs for both the high and the low frequency distribution based on the mean and SD 
#      observed in subject 1.
#   2) take the mean
#   3) repeat that 10000 times
#   4) Look at the distribution of those means across the 10000 samples
d.RT = 
  plyr::rdply(10000,
              mean(rnorm(n = 24, 
                         mean = as.numeric(d1[d1$condition == "high frequency", "mean"]), 
                         sd = as.numeric(d1[d1$condition == "high frequency", "sd"])))) %>%
  mutate(condition = "high frequency") %>%
  rbind(
    plyr::rdply(10000,
                mean(rnorm(n = 24, 
                           mean = as.numeric(d1[d1$condition == "low frequency", "mean"]), 
                           sd = as.numeric(d1[d1$condition == "low frequency", "sd"])))) %>%
      mutate(condition = "low frequency")
  ) %>%
  rename(mean_RT = V1)

d.RT %>%
  ggplot(aes(x = mean_RT)) + geom_histogram()

d.RT %>%
  group_by(condition) %>%
  summarise_at(vars(mean_RT), 
               .funs = list("mean_of_mean" = mean, "sd_of_mean" = sd, 
                            "ci.lower" = function(x) quantile(x, probs = .025),
                            "ci.upper" = function(x) quantile(x, probs = .975))) 
```

See how the standard deviation of the mean across the 10000 samples is the same as the standard error of the mean that we obtained based on SE = SD / sqrt(n). But the sampling-based approach gave us these numbers without requiring us to know this formula.

## Using the sampling-based approach to determine whether the difference between high vs. low frequency is significant

We can use the same approach to ask whether the difference between high and low frequency RTs is significant. For this, we again sample 10000 replications of our data. This time we want to know whether difference between any random combination of a high and low frequency RT is significant.


```{r, echo = T}
# 1) We generate 24 observations each for the high and low frequency condition.
# 2) We subtract the means of the two conditions from each other
# 3) We repeat the process 10000 times
# 4) We look at the distribution of the differences across the 10000 samples

d.diff = 
  plyr::rdply(10000,
              mean(rnorm(n = 24, 
                         mean = as.numeric(d1[d1$condition == "high frequency", "mean"]), 
                         sd = as.numeric(d1[d1$condition == "high frequency", "sd"]))) - 
                mean(rnorm(n = 24, 
                           mean = as.numeric(d1[d1$condition == "low frequency", "mean"]), 
                           sd = as.numeric(d1[d1$condition == "low frequency", "sd"])))) 
d.diff %<>%
  summarise_at(vars(V1), .funs = list("mean_of_diff_of_means" = mean, 
                                      "sd_of_diff_of_means" = sd)) 
d.diff
```

The fact that the mean difference is more than twice (well, 1.96-times) its own standard deviation away from zero tells us that 0 is not contained in the 95\% confidence interval. We would thus feel confident rejecting the null hypothesis that the difference between the high and low frequency condition is 0. In other words, we would say that subject 1 exhibits a significant effect of frequency on processing times. Indeed, the $t$-statistic of an ordinary $t$-test is nothing else but the distance of the mean from zero in terms of its own standard error, i.e, $t=mean / se(mean)=$ `r d.diff[,1]` / `r d.diff[,2]` = `r d.diff[,1] / d.diff[,2]`---a highly significant $t$-value.


In this case, we could have obtained that same conclusion analytically without sampling. The distribution of the difference between two normal distributions is a normal distribution with the mean of the difference equal to the difference between the means (i.e., `r d1[d1$condition == "high frequency", "mean"]` - `r d1[d1$condition == "low frequency", "mean"]` = `r d1[d1$condition == "high frequency", "mean"] - d1[d1$condition == "low frequency", "mean"]`), and the standard deviation of the difference equal to the square root of the sum of variances (i.e., sqrt(`r d1[d1$condition == "high frequency", "sd"]`^2 + `r d1[d1$condition == "low frequency", "sd"]`^2 ) = `r sqrt(d1[d1$condition == "high frequency", "sd"]^2 + d1[d1$condition == "low frequency", "sd"]^2)`), so that the standard error of the mean difference would then be `r sqrt(d1[d1$condition == "high frequency", "sd"]^2 + d1[d1$condition == "low frequency", "sd"]^2)` / sqrt(`r n.trial`) = `r sqrt(d1[d1$condition == "high frequency", "sd"]^2 + d1[d1$condition == "low frequency", "sd"]^2) / sqrt(n.trial)`---matching what we obtained by sampling.
Similarly, the t-test applied to the data from subject 1 yields a result closely resembling that obtained through sampling:

```{r}
t.test(formula = RT ~ condition,
       data = d %>% filter(subject == 1),
       paired = F, var.equal = T)
```

# Removing parametric assumptions: Bootstrap

So far we have seen how to apply sampling *parametrically*. Parametric assumptions---such as the assumption of Normality---can be wrong and this can lead to non-sensical conclusions. This becomes apparent when we look at the accuracy, rather than RTs, of subject 1:

```{r, echo=F, fig.show='hold', out.width="50%", fig.align = "default"}
theme_set(theme_bw())

d %>%
  filter(subject == 1) %>%
  ggplot(aes(x = condition, y = correct)) +
  geom_point(alpha = .3) +
  stat_summary(fun.data = mean_cl_normal, geom = "pointrange", size = 1, color = "blue") +
  scale_y_continuous("Accuracy")
```

Note that the 95\% confidence interval for the accuracy plot extends beyond an accuracy of 1! This is because we *assumed* normality, but proportions are often not normally distributed. Let's ignore that for now. S

One big advantange of the sampling-based approach is that we can remove these assumptions and use *non-parametric* sampling methods. Here, we use the so-called **bootstrap**. When we bootstrap confidence intervals, we essentially re-sample our data (over and over again) to estimate the confidence interval. Just like other sampling-based approaches, bootstrap can be applied to estimate any type of property of our distribution, not just confidence intervals. Unlike the parametric approach, the bootstrap does not assume that our data follows the Normal distribution, and this results in confidence intervals that are more appropriate:

```{r, echo=F, fig.show='hold', out.width="50%", fig.align = "default"}
theme_set(theme_bw())

p = d %>%
  filter(subject == 1) %>%
  ggplot(aes(x = condition, y = RT)) +
  geom_point(alpha = .3) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = 1, color = "blue")

p
p %+% aes(y = correct) +
  scale_y_continuous("Accuracy")
```

Notice, in particular, the **asymmetry in the distance of the upper and lower end of the confidence interval from the mean**.

## How was the confidence interval in the above figure obtained? 

Bootstrapping works almost exactly as the sampling-based approach discussed above, except that we do not assume normality. Instead of sampling from a normal distribution, we sample from the actually observed data (with replacement). Here's an example of drawing *one* bootstrap sample form subject 1. Notice how some trials appear multiple times (and others don't appear at all), for the same number of total trials as in the original experiment:

```{r}
d1 = d %>% 
  filter(subject == 1)

d1 %>% 
  group_by(condition) %>%
  sample_n(n.trial, replace = T)
```

Just as we took many thousands of parametric samples above, we can repeat the bootstrap sampling. Here is the code to obtain the 95\% confidence intervals

```{r, echo = T}
# 1) We *re-sample (with replacement) * 24 observations each for the high and low frequency condition *from the original data*.
# 2) We take the mean for each condition
# 3) We repeat the process 10000 times
# 4) We look at the distribution of the differences across the 10000 samples

d.RT.boot = 
  plyr::rdply(10000,
              d1 %>% 
                group_by(condition) %>%
                sample_n(n.trial, replace = T) %>%
                summarise_at(vars(correct), 
                             .funs = list("mean" = mean)))

d.RT.boot %<>%
  group_by(condition) %>%
  summarise_at(vars(ends_with("mean")), 
               .funs = list("mean_of_mean" = mean, 
                            "ci.lower" = function (x) quantile(x, probs = c(.025)),
                            "ci.upper" = function (x) quantile(x, probs = c(.975))))
  
d.RT.boot
```

This replicates the confidence interval shown in the figure above, including the asymmetry around the mean. **Note *why* this procedure is guaranteed to never yield confidence intervals that fall outside of the 0 to 1 range**: since we are resampling from the actually observed data, it is impossible that the mean of any particular re-sampled sample will be outside of the 0 to 1 range.

And here is the code paralleling the parametric sampling of the mean difference between conditions for both RTs and accuracy (compare it to the parametric code above). For RTs, the parametric sampling-based estimates reported above relatively closely resemble the non-parametric bootstrapped estimates obtained here, though there are some differences. These differences stem from the fact that the RT data are actually *not* normally distributed---contrary to what we assumed above. While the violation of normality is milder for the RT data than for the accuracy data, it still exists and affected the parametric sampling-based estimates (which were `r d.diff[,1]` +/ 1.96 * `r d.diff[,2]` = `r d.diff[,1] - 1.96 * d.diff[,2]` to `r d.diff[,1] + 1.96 * d.diff[,2]`).

```{r, echo = T}
# 1) We *re-sample (with replacement) * 24 observations each for the high and low frequency condition *from the original data*.
# 2) We subtract the means of the two conditions from each other
# 3) We repeat the process 10000 times
# 4) We look at the distribution of the differences across the 10000 samples

d.diff.boot = 
  plyr::rdply(10000,
              d1 %>% 
                group_by(condition) %>%
                sample_n(n.trial, replace = T) %>%
                summarise_at(vars(RT, correct), 
                             .funs = list("diff" = mean)) %>%
                ungroup() %>%
                summarise_at(vars(ends_with("diff")), function(x) -diff(x)))

d.diff.boot %<>%
  select(-.n) %>%
  pivot_longer(ends_with("diff"), names_pattern = "(.*)_diff") %>%
  group_by(name) %>%
  summarise_all(.funs = list("mean_of_diff_of_means" = mean, 
                             "ci.lower" = function (x) quantile(x, probs = c(.025)),
                             "ci.upper" = function (x) quantile(x, probs = c(.975))))
d.diff.boot
```

Bootstrap is sometimes described as making no assumptions. That is wrong. It makes very strong assumptions: that the data you have observed in your experiment (or whatever data set you are bootstrapping over) is representative the underlying population. Indeed, that data set exhaustively describes all possible events that can occur, unlike in parametric simulation approaches. This assumption is less and less problematic the more data we have that we are sampling from (a total of two times `r n.trial` data points from subject 1 would not usually be considered enough). 

# Grouped data

The confidence intervals shown above are obtained under the assumption that the underlying population that we are drawing inferences about is data from subject 1. So, we could now conclude that, *for subject 1*, the RTs for high frequency objects are significantly smaller than for low frequency objects. And, for subject 1, we would not conclude that there is a significant difference in accuracy for low and high frequency objects.

Ultimately though, we tend to be interested in drawing inferences about *people* in general---i.e., across subject, which we believe to have randomly sampled from the overall population. If we want to ask the same question across *all* subjects, we have to take into account that the observations from one subject are not independent of each other. There are many sophisticated ways to do so. One simple way to do so is to calculate the confidence over *by-subject* averages: we first aggregate (summarize) the data down to the subject-level, so that we have one data point per condition per subject; then we ask whether the mean of those by-subject means differs between the two conditions. This way we lose some information, but we are being appropriately conservative since the mean of each condition now is based on only one data point per subject.

The following plot shows all the raw data, the by-subject means (connected by a line for each subject), and the mean and 95\% bootstrapped confidence interval of the by-subject means.

```{r, echo=F, fig.show='hold', out.width="50%", fig.align = "default"}
theme_set(theme_bw())

p = d %>%
  ggplot(aes(x = condition, y = RT, color = subject)) +
  geom_point(alpha = .1, position = position_jitter(height = 0)) +
  geom_point(data = 
               d %>% 
               group_by(subject, condition) %>%
               summarise(RT = mean(RT), correct = mean(correct)),
             size = 2, 
             alpha = .8) +
  geom_line(data = 
              d %>% 
              group_by(subject, condition) %>%
              summarise(RT = mean(RT), correct = mean(correct)),
            aes(group = paste(subject)),
            alpha = .8) +
  stat_summary(data = 
                 d %>% 
                 group_by(subject, condition) %>%
                 summarise(RT = mean(RT), correct = mean(correct)),
               fun.data = mean_cl_boot, geom = "pointrange", size = 1, color = "black")

p
p %+% aes(y = correct) +
  scale_y_continuous("Accuracy")
```

The CIs shown in the above plot are based only on the distribution of the by-subject means within each condition---we estimated the confidence separaretly for each condition. If we want to instead capture that subjects might differ in the extent to which they exhibit an effect of frequency, we should look at the distribution of *by-subject differences* in the means for the high vs. low frequency condition. The colored lines between the left and right columns of the above plots provide an initial visualization of these pair differences. We can see, for example, that there is between-subject variability in the effect of low and high frequency (at least when we look at these differences in raw RTs) and even more between-subject variability in accuracy.

We thus calculate the by-subject difference in the condition means. In this revised plot, we want to know whether the 95\% bootstrapped confidence interval does contain 0. If that is not the case, we would conclude that the effect of frequency is significant after taking into account that our data comes from multiple subjects (assumed to be sampled from the same underlying population that we aim to draw conclusions about).

```{r, echo=F, fig.show='hold', out.width="50%", fig.align = "default", warning=F, message=F}
theme_set(theme_bw())

p = d %>%
  group_by(subject, condition) %>%
  summarise(RT = mean(RT), correct = mean(correct)) %>%
  group_by(subject) %>%
  summarise(RT = diff(RT), correct = diff(correct), condition = "difference") %>%
  ggplot(aes(x = condition, y = RT, color = subject)) +
  geom_point(size = 2, 
             alpha = .8) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", size = 1, color = "black") +
  scale_y_continuous("Difference in RT\n(low vs. high frequency)")

p
p %+% aes(y = correct) +
  scale_y_continuous("Difference in accuracy\n(low vs. high frequency)")
```


# Sampling-based Type I error and power analyses 

Another powerful application of sampling-based approaches is to obtain estimates of the **Type I error** rate (how often the analysis returns significance when in reality there is no effect in the underlying population) or **Type II error** rate of an analysis approach (how often we fail to find an effect that is actually present in the underlying population). Before we proceed, it is important to note that these error rates depend on both the *data* and the *analysis approach*.^[Additionally, Type I and II error rates depend on the assumptions we make while estimating them---a fact that is often overlooked. For example, journals might require a certain minimum of statistical power for publication, but typically do not specify how this power is estimated. We'll return to this point below.] That is, while one approach might yield the targeted Type I error rate (e.g., a Type I error rate of .05 for a significance criterion of $\alpha = .05$), another approach might fail to achieve this error rate on the same data. Conversely, approach A might work better for data set 1, whereas approach B works better for data set 2 (where "better" means achieving an error rate closer to the targeted error rate). 

For Type I errors, we want to avoid in particular *anti-conservativity*---error rates above .05---but also *conservatity*---error rates lower than .05. For **power** (which is simply 1 - Type II error), we want to have as much statistical power as possible. Whereas conventions for targeted Type I error rates tend to be pretty clear, conventions for what constitutes sufficient power tend to be less clear. 

In this section, we go through how one can use the sampling-based approach to assess power and Type I error rates. These analyses are based conducted before collecting data (e.g., so that one can still change how much data to collect), but can also be conducted after data collection prior to the analysis (e.g., to guide which analysis approach one should choose), or even after analyses are already conducted (e.g., to assess how much we can trust our results).

The basic idea of the sampling-based approach to Type I error or power analysis is that we 

  (i) determine our assumed 'ground truth', i.e., the assumptions we want to make about the effects in the underlying population that we are interested in.
  (ii) generate hypothetical results (e.g., data we might obtain from an experiment we plan to run, or have run) based on the assumed ground truth. This data generation could employ a *parametric* or *non-parametric* process (e.g., bootstrap).
  (iii) analyze the hypothetical results with the intended analysis approach (or approach*es* if we want to compare different approaches).
  (iv) repeat (ii) and (iii) many times. 
  (v) calculate how often we find a significant effect.
  
The only difference between Type I error and power simulations is in step (i)---whether we assume a null effect or an effect different from zero. If we generate the data under the assumption of a null effect, then the proportion of significant effects in step (v) provides an estimate of the Type I error rate. If, on the other hand, we generate the data under a non-null effect, then the proportion of significant effects in step (v) provides an estimate of the power to detect an effect of that size.

## Parametrically estimating Type I error
```{r, include=FALSE}
# Which subject is chosen for simulation?
which.subj = 3
```

Let's say we want to analyze both the RT and the accuracy data from our experiment with a $t$-test. Being conservative researchers, we are interested in estimating the Type I error rate of this approach. We don't want to be anti-conservative, as this would mean we might draw conclusions that are unlikely to hold, and describe result patterns that are unlikely to replicate. To illustrate this process, we again start with a scenario in which we have data from only one subject )this time, we'll go with Subject ```r which.subj```). We first get the mean and SD for both RTs and accuracy from our data. 

### An example

If we apply a standard $t$-test to the original RT and accuracy data, we find a (highly) significant diffference in RTs but not in accuracy:

```{r, echo=FALSE}
d.subj = d %>% filter(subject == which.subj)
t.test(RT ~ condition, data = d.subj, var.equal = T, paired = F)
t.test(correct ~ condition, data = d.subj, var.equal = T, paired = F)
```

For **step (i)** of our Type I error calculation, we take the observed mean and standard deviation (SD) of the data in our sample, and then make a function that generates data with those parameters under, e.g., the assumption of normality. For this, we ignore condition, since we want to simulate a scenario in which the ground truth is that there is *no* difference between the two conditions (i.e., a null effect):

```{r, echo=FALSE}
d.pars = d.subj %>% 
  summarise_at(vars(RT, correct), .funs = list("mean" = mean, "sd" = sd))

d.pars
```

For **step (ii)**, we write a function that can generate RT and accuracy data from these parameters. For the RTs we assume that they are drawn from a normal distribution with the mean and SD observed in our experiment for Subject ```r which.subj```. For accuracy, we assume that it is drawn from a Bernoulli distribution with the mean observed in our experiment for Subject ```r which.subj```.

```{r, echo = T}
# A function to parametrically generate data from our thought
# experiment:
make_single_subject_data = function(n.trial, d.pars) {
  # This generates a data frame with one row for each unique
  # combination of the variable values handed to it. E.g., if
  # there are 24 trials (n.trial = 24), we will end up with 
  # 48 rows (= 1 subject * 2 conditions * 48 trials)
  crossing(
    subject = 1,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
    # Sample the responses / outcomes / dependent variables, i.e.
    # the accuracy and RTs. d.pars is a data frame with the mean
    # and SD of the RT and accuracy distributions we want to sample
    # from.
    mutate(
      correct = rbinom(nrow(.), 1, d.pars$correct_mean[1]),
      RT = rnorm(nrow(.), d.pars$RT_mean[1], d.pars$RT_sd[1])
    ) %>%
    # Do some formatting and sort the data by subject, trial, and 
    # condition. These lines of code are not necessary.
    as_tibble() %>%
    mutate_at(c("condition", "subject"), factor) %>%
    arrange(subject, trial, condition)
}
```

Here's an example draw from that function, giving us one random instance of our thought experiment. Notice that the RT and accuracy values for the two conditions are not identical. That's to be expected since each trial is generated as a draw from a random variable. The critical question is how this affects our conclusions about significant differences between the means of the conditions.

```{r}
# Let's sample some data from our parametric generative process (step ii)
make_single_subject_data(n.trial, d.pars)
```

For **step (iii)**, we write a function that applies the two $t$-tests to the data set, and stores the $p$-value for each test in a data.frame. To illustrate how this function works, we apply it to the original data and to one example data generated by our data generation process:

```{r}
# a function that applies a t-test to both the RT and the 
# accuracy data and then stores the two p-values in a data
# frame. We'll be using this function for all of the examples
# below.
do_test = function(data) {
  d = tibble(.rows = 1)
  
  if ("RT" %in% names(data)) {
    t1 = t.test(RT ~ condition, data = data, var.equal = T)
    d %<>%
      mutate(RT.p.value = t1$p.value)
  }
  if ("correct" %in% names(data)) {
    t2 = t.test(correct ~ condition, data = data, var.equal = T)
    d %<>%
      mutate(Accuracy.p.value = t2$p.value)
  }

  return(d)
}

# Apply our test function to the actual data (the individual subject we 
# selected above).
do_test(d.subj)

# One draw from the data generated under the assumption of a null effect
# that is then analyzed in our do_test() function.
make_single_subject_data(n.trial, d.pars) %>%
  do_test()
```

For **step (iv)**, we repeat the step (ii) and (iii) 1000 times, giving us the following (showing only the first 20 rows):

```{r, echo=FALSE, warning=FALSE}
# rdply is a function that repeats some r code (the second argument)
# .n-times (the first argument), and then stores the results in a data
# frame, with one row for each of the .n outputs that result from run-
# ning each of the .n repetitions of the r code.
d.type1 = 
  plyr::rdply(
    # How many simulations/samples do we want to generate and analyze?
    .n = n.samples,
    # What we want to be repeated:
    function(i) {
      dd = try(
        make_single_subject_data(n.trial, d.pars)  %>%
          do_test(), silent = TRUE)
      # t.test sometimes fail. I'm catching those cases
      # and setting the p.values to NA
      if (class(dd) == "try-error")
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    })

# Show the first 20 rows of the resulting data frame with all the 
# simulation results
d.type1 %>% 
  head(20)
```

Finally, for **step (v)**, we calculate the proportion of the 1000 samples for which the effects of conditions on RTs or accuracy were significant, which corresponds to the estimated Type I error rate of the $t$-tests:

```{r, echo=FALSE}
# define pipe (since we'll be reusing it)
get_TypeI = . %>%
  dplyr::summarise_at(
    vars(ends_with("p.value")), 
    function(x) round(mean(x < .05, na.rm = T), 3)) %>%
  rename_all(.funs = ~ gsub("p\\.value", "Type_I", .x))

d.type1 %>%
  get_TypeI()
```

These Type I error rates look OKish for the RT analysis, but clearly conservative for the analysis of accuracy. 

As mentioned in the footnote above, any Type I error calculation is only as good as its assumptions. In this particular case, we've made some problematic assumptions. For example, we assumed that RTs are drawn from a normal distribution, but that would predict that there can be negative RTs (the tails of a normal distribution are infinitely long). Such problematic assumptions do not *necessarily* lead to wrong estimates of the Type I error rate, but they *can*. 

## The consequences of making problematic assumptions about the ground truth (step (i))

In this particular example, we actually *know* the ground truth. That's because the data analyzed here are actually not data from a psycholinguistic experiment. Instead, we generated them silently at the top of this document, using a lognormal (rather than normal) distribution for the RTs and a Bernoulli distribution for the accuracies (plus normally distributed individual differences across the subjects). This means that we can compare the Type I error rate of the $t$-test obtained above to the Type I error rate of the same $t$-test if the data is generated following the ground truth. 

The following repeats steps (ii) - (v) under the correct ground truth. For RTs, we find that we get a pretty similar Type I error rate as above, despite the fact that our assumptions have changed. For the analysis of accuracy, however, we see a substantially lower Type I error rate than the target of .05. That is, the $t$-test is actually rather conservative for this particular data set.

```{r, echo=FALSE}
# A function to generate ground truth data
make_from_original_data = function(n.trial, which.subject) {
  crossing(
    subject = which.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
    mutate(
      muLogOddsCorrect = mean(d.orig[d.orig$subject == which.subject, 
                                     "muLogOddsCorrect"][[1]]),
      muLogRT = mean(d.orig[d.orig$subject == which.subject, 
                            "muLogRT"][[1]])
    ) %>%
    group_by(subject) %>%
    mutate(
      muLogOddsCorrect.bySubject = mean(d.orig[d.orig$subject == which.subject, 
                                               "muLogOddsCorrect.bySubject"][[1]]),
      muLogRT.bySubject = mean(d.orig[d.orig$subject == which.subject, 
                                      "muLogRT.bySubject"][[1]])
    ) %>%
    rowwise() %>%
    mutate(
      correct = rbinom(1, 1, 
                       plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
      RT = round(100 + 
                   exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
    ) %>%
    as_tibble() %>%
    select(-starts_with("mu")) %>%
    mutate_at(c("condition", "subject"), factor) %>%
    arrange(subject, trial, condition)
}

d.type1.true = 
  plyr::rdply(
    .n = n.samples,
    function(i) {
      dd = try(
        make_from_original_data(n.trial, which.subject = which.subj) %>%
          do_test(), silent = TRUE)
      if (class(dd) == "try-error")
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    })

d.type1.true %>%
  get_TypeI()
```

These results suggest that we should not use a $t$-test for the analysis of our accuracy results. Indeed, this problem of $t$-tests and ANOVA for the analysis of categorical outcomes is well-known. But even if we didn't know anything about it, the sampling-based approach has provided us with a tool to notice that something is off.

## What if we don't know the ground truth? Bootstrap!

In the above example, we knew the ground truth and this let us assess whether the assumptions we made for our Type I error analysis were problematic. But that is rarely the case. How then, can we avoid problematic assumptions? While there is no magic bullet---nothing that will always work---the non-parametric bootstrap we've used above to obtain confidence intervals also allows us to get Type I error estimates, without assuming, for example, normality. Specifically, we can bootstrap from the data we have. Specifically, we can substitute steps (i) and (ii) by sampling without replacement data points from subject 2. 

```{r, echo=TRUE}
# A bootstrap function to generate data
bootstrap_single_subject_data = function(n.trial, d) {
  d %>%
    ungroup() %>%
    # Sample twice as many rows as n.trials since there are two conditions
    sample_n(n.trial * 2, replace = T) %>%
    # Randomly assign data to conditions (since we are simulating a null effect)
    mutate(condition = rep(c("high frequency", "low frequency"), n.trial))
}

bootstrap_single_subject_data(n.trial, d.subj)
```

Applying the same steps (iii) - (v) as above, we find that the Type I error rate of both analyses seems acceptable:

```{r}
d.type1.boot = 
  plyr::rdply(
    .n = n.samples,
    function(i) {
      dd = try(bootstrap_single_subject_data(n.trial, d.subj) %>%
                 do_test(), silent = TRUE)
      if (class(dd) == "try-error")
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    }) 

d.type1.boot %>%
  get_TypeI()
```


## Power

To calculate power, we proceed in parallel to the approach described above for Type I error rates. For power, too, we can use parametric or non-parametric approaches. The following example shows the parametric approach to simulated the data from Subject ```r which.subj```. We can modify the function used above, so that it can accommodate both Type I error analyses (for effect = 0) and power analyses (for effects != 0). For the sake of simplicity, we focus on only RTs.

```{r}
# A function to parametrically generate data with an effect
make_single_subject_data = function(n.trial, d.pars, effect = 0) {
  crossing(
    subject = 1,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    RT = rnorm(
      nrow(.), 
      # Here we are adding the effect to the mean of the normal 
      # distribution we are sampling from:
      d.pars$RT_mean[1] + ifelse(condition == "low frequency", -effect, +effect),
      d.pars$RT_sd[1])
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition)
}
```

Here's an example draw from this revised function, for a hypothetical (huge) difference of 250 milliseconds between the means of the two conditions:

```{r}
make_single_subject_data(n.trial, d.pars, effect = 250)
```

With this simple change in the data generation---i.e., step (i)---we can calculate the power for, e.g., a combination of different amounts of data (12, 24, 48, or 96 trials) and different effect sizes (25, 50, and 100 ms):

```{r, echo=TRUE}
get_power = . %>%
  get_TypeI() %>%
  rename_all(., .funs = ~gsub("Type_I", "Power", .x))

sample_power = function(d.pars, n.trials, effects, n.samples = n.samples) {
  d = tibble(.rows = 0)
  for (n.trial in n.trials) {
    for (effect in effects) {
      d %<>% rbind(
        plyr::rdply(
          .n = n.samples,
          function(i) {
            dd = try(
              make_single_subject_data(n.trial, d.pars, effect = effect)  %>%
                do_test(), silent = TRUE)
            if (class(dd) == "try-error")
              dd = data.frame(RT.p.value = NA)
            return(dd)
          }) %>%
          mutate(effect = effect, n.trial = n.trial))
    }
  }
  
  d %<>%
    group_by(effect, n.trial) %>%
    get_power()
  
  return(d)
}

sample_power(
  n.trials = c(12, 24, 48, 96),
  effects = c(25, 50, 100),
  d.pars = d.pars,
  n.samples = 200
) %>%
  ggplot(aes(x = n.trial, y = RT.Power, color = factor(effect))) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  stat_summary(fun = mean, geom = "line") +
  # Adding a line to indicate the 'power' we'd get from the Type I error
  # alone
  geom_hline(yintercept = .05, linetype = 2, color = "darkgray") +
  scale_x_continuous("Number of trials") +
  scale_y_continuous("Power", limits = c(0,1)) +
  scale_color_discrete("Effect size") + theme_bw()
```


### Power in grouped data

The same approach can be applied to grouped data. Here is an example using a generation function that is similar to the one used to generate the (simulated) results that we introduced at the beginning of this document. We can then ask how the power of a *paired* $t$-test over the by-participant means of RTs and accuracy scales as a function of the number of trials and the number of subjects. If you are working with similar repeated-measures data, have a look at the code in the R markdown source.

Note how we stand to gain much less power from increasing the number of trials per subjects, compared to increasing the number of subjects (e.g., 4 subjects with 8 trials each is the same amount of total data as 8 subjects with 4 trials each, but the latter gives us more power). Of course, this depends on the relative variability *within* vs. *between* subjects, but the scenario depicted here holds for many of our experiments.

```{r, echo=FALSE}
make_multi_subject_data = function(n.subject, n.trial, 
                                   effect.RT = 0, effect.correct = 0) {
  crossing(
    subject = 1:n.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "low frequency" ~ plogis(qlogis(.8) - effect.correct / 2),
      condition == "high frequency" ~ plogis(qlogis(.8) + effect.correct / 2)
    )),
    muLogRT = case_when(
      condition == "low frequency" ~ log(exp(6.3) + effect.RT / 2),
      condition == "high frequency" ~ log(exp(6.3) - effect.RT / 2)
    )
  ) %>%
  group_by(subject, condition) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, .5),
    muLogRT.bySubject = rnorm(1, 0, .1)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(100 + exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition) %>%
    select(-starts_with("mu"))
}

do_multi_subject_test = function(data) {
  data %<>%
    group_by(subject, condition) %>%
    summarise_at(c("RT", "correct"), mean)
  
  d = tibble(.rows = 1)
  if ("RT" %in% names(data)) {
    t1 = t.test(RT ~ condition, data = data, var.equal = T, paired = T)
    d %<>%
      mutate(RT.p.value = t1$p.value)
  }
  if ("correct" %in% names(data)) {
    t2 = t.test(correct ~ condition, data = data, var.equal = T, paired = T)
    d %<>%
      mutate(Accuracy.p.value = t2$p.value)
  }

  return(d)
}

sample_power = function(n.subjects, n.trials, n.samples = n.samples) {
  d = tibble(.rows = 0)
  for (n.subject in n.subjects) {
    for (n.trial in n.trials) {
      d %<>% rbind(
        plyr::rdply(
          .n = n.samples,
          function(i) {
            dd = try(
              make_multi_subject_data(n.subject, n.trial, 
                                      effect.RT = 25,
                                      effect.correct = 1)  %>%
                do_multi_subject_test(), silent = TRUE)
            if (class(dd) == "try-error")
              dd = data.frame(RT.p.value = NA)
            return(dd)
          }) %>%
          mutate(n.subject = n.subject, n.trial = n.trial))
    }
  }
  
  d %<>%
    group_by(n.subject, n.trial) %>%
    get_power()
  
  return(d)
}

sample_power(n.subjects = c(4, 8, 16, 32), n.trials = c(4, 8, 16, 32), 
             n.samples = 300) %>%
  pivot_longer(
    cols = c("RT.Power", "Accuracy.Power"),
    values_to = "Power",
    names_pattern = "(.*)\\.Power",
    names_to = "DV"
  ) %>%
  mutate(DV = factor(DV, levels = c("RT", "Accuracy"))) %>%
  ggplot(aes(x = n.subject, y = Power, color = factor(n.trial))) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", alpha = .5) +
  stat_summary(fun = mean, geom = "line", alpha = .5) +
  # Adding a line to indicate the 'power' we'd get from the Type I error
  # alone
  geom_hline(yintercept = .05, linetype = 2, color = "darkgray") +
  scale_x_continuous("Number of subject") +
  scale_y_continuous("Power", limits = c(0,1)) +
  scale_color_discrete("Number\nof trials") + theme_bw() +
  facet_wrap(~DV)
```

# Researcher's degrees of freedom: a sampling-based case study

Let's say we collect 30 subjects' worth of data (with 24 trials each), from the same hypothetical experiment used in the power analyses right above. We then conduct a paired $t$-test on both the RTs and the accuracy:

```{r}
d = make_multi_subject_data(n.trial = 16, n.subject = 24) 

d %>%
  do_multi_subject_test()
```

The result of the $t$-test for the reaction times looks 'promising'. They are significant, and one might say it is "approaching significance". So we decide that we need more statistical power to understand the effect. We collect data from 8 more subjects and then test again:

```{r}
d %<>%
  rbind(
    make_multi_subject_data(n.trial = 16, n.subject = 8))

d %>%
  do_multi_subject_test()
```

It seems like we were on the right track! There is indeed a significant effect of high vs. low frequency on reaction times. We decide to submit the result for publication. 

Eight years later, a team of undergraduate researchers decides to replicate our study. They succeed in following the same design, procedure, etc., and sample from the same underlying population. They are weary of the small number of trials and subjects. They decided to run twice the number of trials per subject (32) and four times the number of subjects of the original study (128), for a total of *eight times the original data*. Using the same paired $t$-test approach, they find:

```{r}
make_multi_subject_data(n.trial = 32, n.subject = 128) %>%
  do_multi_subject_test()
```

Which study should we trust? A failure to replicate is *not necessarily* due to any fault of the original researchers (or the replication team): even if the test we employ achieves the targeted Type I error rate of 5%, one in twenty experiments will return a significant effect even if the underlying effect is null. 

But is there something that the original team could have done better? If you don't know the answer, make sure to read Nelson et al. (2008) article in the *Annual Review of Psychology*.

## Illustrating the consequences of the run-test-run-test-... scheme (incremental testing)

Let's use the sampling-based approach to investigate the consequences of this incremental approach. We first write a function that collects data in 10 steps (of 10 subjects with 16 trials each), each time testing all the data collected so far. 

In the output, we see the $p$-values for each batch of 10 subjects, both for the $t$-test on RTs and the $t$-test on accuracy. Next to it, we see the column `RT.significant`, indicating whether the $p$-value for the RT analysis was significant (< .05) *in that batch*. Next to that, the columns `RT.significant.incremental` tells us whether the $p$-value *in this or any preceding batch* was significant. This captures the **incremental testing** approach, in which we would stop any time we have found significance. For example, even if I had planned to in theory collect data from up to 10 batches of 10 subjects (testing for an effect after each batch has been collected), under the incremental testing approach I'd stop as soon as I found significance on any of the earlier batches. I'd then conclude and report a significant effect. That's why the column `RT.significant.incremental` has the value `TRUE` for any batch that follow a batch on which I have found significance.

```{r}
run_test_run_test = function(n.trial = 16, n.subject = 10, n.batch = 10) {
  d = plyr::rdply(
    .n = n.batch, .id = "batch",
    make_multi_subject_data(n.trial = n.trial, n.subject = n.subject)) %>%
    group_by(batch, subject, condition) %>%
    dplyr::summarise_at(c("correct", "RT"), mean)
  
  d.t = tibble(.rows = 0)
  for (i in 1:max(d$batch)) {
    d.t %<>%
      rbind(
      do_multi_subject_test(d %>%
                              filter(batch <= i)) %>%
        mutate(batch = i))
  }
  
  d.t %>%
    mutate(
      RT.significant = RT.p.value < .05,
      RT.significant.incremental = cumany(RT.significant),
      Acc.significant = Accuracy.p.value < .05,
      Acc.significant.incremental = cumany(Acc.significant)
      )
}

run_test_run_test()
```

Now we can ask what happens if we repeated this thought experiment many times. The black line in the following plot shows the estimated Type I error rate if we plan how many subjects we run and only test once (e.g., we might plan to run 6 batches, i.e., 60 subjects and then test and stop, regardless of the result we find). The blue line shows the Type I error rate if we test after every batch of subject and stop if we find significance.

```{r}
d.sim = plyr::rdply(
  .n = 500,
  run_test_run_test()
) 

d.sim %>%
  group_by(.n, batch) %>%
  summarise(
    RT.TypeI.cumulative = mean(RT.significant),
    Acc.TypeI.cumulative = mean(Acc.significant),
    RT.TypeI.incremental = mean(RT.significant.incremental),
    Acc.TypeI.incremental = mean(Acc.significant.incremental)) %>%
  ggplot(aes(x = batch, y = RT.TypeI.incremental)) +
  stat_summary(fun = mean, geom = "line", color = "blue") +
  stat_summary(fun = mean, geom = "line", color = "black", aes(y = RT.TypeI.cumulative)) +
  # Adding a line to indicate the 'power' we'd get from the Type I error
  # alone
  geom_hline(yintercept = .05, linetype = 2, color = "darkgray") +
  scale_x_continuous("Number of batches of subjects (10 each) administered") +
  scale_y_continuous("Type I error", limits = c(0,1)) + theme_bw()
```

This illustrates the problem---an inflated Type I error---of the incremental testing approach. Simmons et al. (2011) in *Psychological Science*, summarized also in the Nelson et al. (2018) article, discuss incremental testing as one of several common practices and researchers' degrees of freedom that have under-appreciated consequences on the Type I error of our studies.