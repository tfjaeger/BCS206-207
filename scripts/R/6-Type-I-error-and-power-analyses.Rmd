---
title: "Sampling-based Type I and power analyses"
author: "T. Florian Jaeger"
date: "3/15/2021"
output: 
  pdf_document: default
  bookdown::gitbook:
   lib_dir: assets
   split_by: none
   config:
    toolbar:
      position: static
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  results = "markup", warning = FALSE, cache = TRUE,
  fig.align = "center", fig.width = 6.5)
```

```{r constants, include = F}
n.samples = 2000
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("boot")       # for automatic bootstrap
```

# This document
This document is created in R markdown. R markdown combines text with R code, allowing us to see the code and its output, embedded within the text describing the code. If you have the original R markdown file (file extension .Rmd), you can 'knit' the document into an HMTL, PDF, or Word file. Just be aware that the code executes a good number of sampling-based simulations, and these simulations take some time to complete. The parameter that determines how many samples are used in each of the sampling simulations is `n.samples`. You can change the value of `n.samples` to a small number (e.g., `10`) to speed up compilation.

# An example problem

In this document, we are using the same example data set and problem already introduced in the lecture on sampling-based approaches to confidence intervals.

```{r, include = F}
# We will generate some fake data to work with, but you could plug your own real data in here 
# (and change the code below so that it matches the names of the variables in your data)
set.seed(123456)

n.subject = 48
n.trial = 24

# Let's make some (fake) data
d.orig = 
  crossing(
    subject = 1:n.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "low frequency" ~ .78,
      condition == "high frequency" ~ .91
    )),
    muLogRT = case_when(
      condition == "low frequency" ~ 6.5,
      condition == "high frequency" ~ 6.1
    )
  ) %>%
  group_by(subject, condition) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, .75),
    muLogRT.bySubject = rnorm(1, 0, .4)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(100 + exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition)

d = d.orig %>%
    select(-starts_with("mu"))
```

Let's say we have an experiment with `r n.subject` subjects, in which each subject saw `r n.trial` trials for each of two conditions (for a total of `r n.trial * 2` trials per subject). On each trial, subjects saw a picture of an object and had to decide as quickly and accurately whether the picture depicts something animate (a living thing) or not. The two conditions manipulate whether the object is something that we see frequently in our life ("high frequency") or not ("low frequency"). We collected both reaction times of subjects' responses (`RT`), and whether those responses were accurate (`correct`). We hypothesize that more frequent objects will be responded to more quickly and accurately. 


# Sampling-based Type I error and power analyses 

Another powerful application of sampling-based approaches is to obtain estimates of the **Type I error** rate (how often the analysis returns significance when in reality there is no effect in the underlying population) or **Type II error** rate of an analysis approach (how often we fail to find an effect that is actually present in the underlying population). Before we proceed, it is important to note that these error rates depend on both the *data* and the *analysis approach*.^[Additionally, Type I and II error rates depend on the assumptions we make while estimating them---a fact that is often overlooked. For example, journals might require a certain minimum of statistical power for publication, but typically do not specify how this power is estimated. We'll return to this point below.] That is, while one approach might yield the targeted Type I error rate (e.g., a Type I error rate of .05 for a significance criterion of $\alpha = .05$), another approach might fail to achieve this error rate on the same data. Conversely, approach A might work better for data set 1, whereas approach B works better for data set 2 (where "better" means achieving an error rate closer to the targeted error rate). 

For Type I errors, we want to avoid in particular *anti-conservativity*---error rates above .05---but also *conservatity*---error rates lower than .05. For **power** (which is simply 1 - Type II error), we want to have as much statistical power as possible. Whereas conventions for targeted Type I error rates tend to be pretty clear, conventions for what constitutes sufficient power tend to be less clear. 

In this section, we go through how one can use the sampling-based approach to assess power and Type I error rates. These analyses are based conducted before collecting data (e.g., so that one can still change how much data to collect), but can also be conducted after data collection prior to the analysis (e.g., to guide which analysis approach one should choose), or even after analyses are already conducted (e.g., to assess how much we can trust our results).

The basic idea of the sampling-based approach to Type I error or power analysis is that we 

  (i) determine our assumed 'ground truth', i.e., the assumptions we want to make about the effects in the underlying population that we are interested in.
  (ii) generate hypothetical results (e.g., data we might obtain from an experiment we plan to run, or have run) based on the assumed ground truth. This data generation could employ a *parametric* or *non-parametric* process (e.g., bootstrap).
  (iii) analyze the hypothetical results with the intended analysis approach (or approach*es* if we want to compare different approaches).
  (iv) repeat (ii) and (iii) many times. 
  (v) calculate how often we find a significant effect.
  
The only difference between Type I error and power simulations is in step (i)---whether we assume a null effect or an effect different from zero. If we generate the data under the assumption of a null effect, then the proportion of significant effects in step (v) provides an estimate of the Type I error rate. If, on the other hand, we generate the data under a non-null effect, then the proportion of significant effects in step (v) provides an estimate of the power to detect an effect of that size.

## Parametrically estimating Type I error

```{r, include=FALSE}
# Which subject is chosen for simulation?
which.subj = 3
```

Let's say we want to analyze both the RT and the accuracy data from our experiment with a $t$-test. Being conservative researchers, we are interested in estimating the Type I error rate of this approach. We don't want to be anti-conservative, as this would mean we might draw conclusions that are unlikely to hold, and describe result patterns that are unlikely to replicate. To illustrate this process, we again start with a scenario in which we have data from only one subject (this time, we'll go with Subject ```r which.subj```). We first get the mean and SD for both RTs and accuracy from our data. 

### An example

If we apply a standard $t$-test to the original RT and accuracy data, we find a (highly) significant difference in RTs but not in accuracy:

```{r, echo=FALSE}
d.subj = d %>% filter(subject == which.subj)
t.test(RT ~ condition, data = d.subj, var.equal = T, paired = F)
t.test(correct ~ condition, data = d.subj, var.equal = T, paired = F)
```

For **step (i)** of our Type I error calculation, we take the observed mean and standard deviation (SD) of the data in our sample, and then make a function that generates data with those parameters under, e.g., the assumption of normality. For this, we ignore condition, since we want to simulate a scenario in which the ground truth is that there is *no* difference between the two conditions (i.e., a null effect):

```{r, echo=FALSE}
d.pars = d.subj %>% 
  summarise_at(vars(RT, correct), .funs = list("mean" = mean, "sd" = sd))

d.pars
```

For **step (ii)**, we write a function that can generate RT and accuracy data from these parameters. For the RTs we assume that they are drawn from a normal distribution with the mean and SD observed in our experiment for Subject ```r which.subj```. For accuracy, we assume that it is drawn from a Bernoulli distribution with the mean observed in our experiment for Subject ```r which.subj```.

```{r, echo = T}
# A function to parametrically generate data from our thought
# experiment:
make_single_subject_data = function(n.trial, d.pars) {
  # This generates a data frame with one row for each unique
  # combination of the variable values handed to it. E.g., if
  # there are 24 trials (n.trial = 24), we will end up with 
  # 48 rows (= 1 subject * 2 conditions * 48 trials)
  crossing(
    subject = 1,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
    # Sample the responses / outcomes / dependent variables, i.e.
    # the accuracy and RTs. d.pars is a data frame with the mean
    # and SD of the RT and accuracy distributions we want to sample
    # from.
    mutate(
      correct = rbinom(nrow(.), 1, d.pars$correct_mean[1]),
      RT = rnorm(nrow(.), d.pars$RT_mean[1], d.pars$RT_sd[1])
    ) %>%
    # Do some formatting and sort the data by subject, trial, and 
    # condition. These lines of code are not necessary.
    as_tibble() %>%
    mutate_at(c("condition", "subject"), factor) %>%
    arrange(subject, trial, condition)
}
```

Here's an example draw from that function, giving us one random instance of our thought experiment. Notice that the RT and accuracy values for the two conditions are not identical. That's to be expected since each trial is generated as a draw from a random variable. The critical question is how this affects our conclusions about significant differences between the means of the conditions.

```{r}
# Let's sample some data from our parametric generative process (step ii)
make_single_subject_data(n.trial, d.pars)
```

For **step (iii)**, we write a function that applies the two $t$-tests to the data set, and stores the $p$-value for each test in a data.frame. To illustrate how this function works, we apply it to the original data and to one example data generated by our data generation process:

```{r}
# a function that applies a t-test to both the RT and the 
# accuracy data and then stores the two p-values in a data
# frame. We'll be using this function for all of the examples
# below.
do_test = function(data) {
  d = tibble(.rows = 1)
  
  if ("RT" %in% names(data)) {
    t1 = t.test(RT ~ condition, data = data, var.equal = T)
    d %<>%
      mutate(RT.p.value = t1$p.value)
  }
  if ("correct" %in% names(data)) {
    t2 = t.test(correct ~ condition, data = data, var.equal = T)
    d %<>%
      mutate(Accuracy.p.value = t2$p.value)
  }

  return(d)
}

# Apply our test function to the actual data (the individual subject we 
# selected above).
do_test(d.subj)

# One draw from the data generated under the assumption of a null effect
# that is then analyzed in our do_test() function.
make_single_subject_data(n.trial, d.pars) %>%
  do_test()
```

For **step (iv)**, we repeat the step (ii) and (iii) 1000 times, giving us the following (showing only the first 20 rows):

```{r, echo=FALSE, warning=FALSE}
# rdply is a function that repeats some r code (the second argument)
# .n-times (the first argument), and then stores the results in a data
# frame, with one row for each of the .n outputs that result from run-
# ning each of the .n repetitions of the r code.
d.type1 = 
  plyr::rdply(
    # How many simulations/samples do we want to generate and analyze?
    .n = n.samples,
    # What we want to be repeated:
    function(i) {
      dd <- try(
        make_single_subject_data(n.trial, d.pars)  %>%
          do_test(), 
        silent = TRUE)
      # t.test sometimes fail. I'm catching those cases
      # and setting the p.values to NA
      if (any(class(dd) == "try-error"))
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    })

# Show the first 20 rows of the resulting data frame with all the 
# simulation results
d.type1 %>% 
  head(20)
```

Finally, for **step (v)**, we calculate the proportion of the 1000 samples for which the effects of conditions on RTs or accuracy were significant, which corresponds to the estimated Type I error rate of the $t$-tests:

```{r, echo=FALSE}
# define pipe (since we'll be reusing it)
get_TypeI = . %>%
  dplyr::summarise_at(
    vars(ends_with("p.value")), 
    function(x) round(mean(x < .05, na.rm = T), 3)) %>%
  rename_all(.funs = ~ gsub("p\\.value", "Type_I", .x))

d.type1 %>%
  get_TypeI()
```

These Type I error rates look OKish for the RT analysis, but clearly conservative for the analysis of accuracy. 

As mentioned in the footnote above, any Type I error calculation is only as good as its assumptions. In this particular case, we've made some problematic assumptions. For example, we assumed that RTs are drawn from a normal distribution, but that would predict that there can be negative RTs (the tails of a normal distribution are infinitely long). Such problematic assumptions do not *necessarily* lead to wrong estimates of the Type I error rate, but they *can*. 

## The consequences of making problematic assumptions about the ground truth (step (i))

In this particular example, we actually *know* the ground truth. That's because the data analyzed here are actually not data from a psycholinguistic experiment. Instead, we generated them silently at the top of this document, using a lognormal (rather than normal) distribution for the RTs and a Bernoulli distribution for the accuracies (plus normally distributed individual differences across the subjects). This means that we can compare the Type I error rate of the $t$-test obtained above to the Type I error rate of the same $t$-test if the data is generated following the ground truth. 

The following repeats steps (ii) - (v) under the correct ground truth. For RTs, we find that we get a pretty similar Type I error rate as above, despite the fact that our assumptions have changed. For the analysis of accuracy, however, we see a substantially lower Type I error rate than the target of .05. That is, the $t$-test is actually rather conservative for this particular data set.

```{r, echo=FALSE}
# A function to generate ground truth data
make_from_original_data = function(n.trial, which.subject) {
  crossing(
    subject = which.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
    mutate(
      muLogOddsCorrect = mean(d.orig[d.orig$subject == which.subject, 
                                     "muLogOddsCorrect"][[1]]),
      muLogRT = mean(d.orig[d.orig$subject == which.subject, 
                            "muLogRT"][[1]])
    ) %>%
    group_by(subject) %>%
    mutate(
      muLogOddsCorrect.bySubject = mean(d.orig[d.orig$subject == which.subject, 
                                               "muLogOddsCorrect.bySubject"][[1]]),
      muLogRT.bySubject = mean(d.orig[d.orig$subject == which.subject, 
                                      "muLogRT.bySubject"][[1]])
    ) %>%
    rowwise() %>%
    mutate(
      correct = rbinom(1, 1, 
                       plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
      RT = round(100 + 
                   exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
    ) %>%
    as_tibble() %>%
    select(-starts_with("mu")) %>%
    mutate_at(c("condition", "subject"), factor) %>%
    arrange(subject, trial, condition)
}

d.type1.true = 
  plyr::rdply(
    .n = n.samples,
    function(i) {
      dd = try(
        make_from_original_data(n.trial, which.subject = which.subj) %>%
          do_test(), silent = TRUE)
      if (any(class(dd) == "try-error"))
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    })

d.type1.true %>%
  get_TypeI()
```

These results suggest that we should not use a $t$-test for the analysis of our accuracy results. Indeed, this problem of $t$-tests and ANOVA for the analysis of categorical outcomes is well-known. But even if we didn't know anything about it, the sampling-based approach has provided us with a tool to notice that something is off.

## What if we don't know the ground truth? Bootstrap!

In the above example, we knew the ground truth and this let us assess whether the assumptions we made for our Type I error analysis were problematic. But that is rarely the case. How then, can we avoid problematic assumptions? While there is no magic bullet---nothing that will always work---the non-parametric bootstrap we've used above to obtain confidence intervals also allows us to get Type I error estimates, without assuming, for example, normality. Specifically, we can bootstrap from the data we have. Specifically, we can substitute steps (i) and (ii) by sampling without replacement data points from subject 2. 

```{r, echo=TRUE}
# A bootstrap function to generate data
bootstrap_single_subject_data = function(n.trial, d) {
  d %>%
    ungroup() %>%
    # Sample twice as many rows as n.trials since there are two conditions
    sample_n(n.trial * 2, replace = T) %>%
    # Randomly assign data to conditions (since we are simulating a null effect)
    mutate(condition = rep(c("high frequency", "low frequency"), n.trial))
}

bootstrap_single_subject_data(n.trial, d.subj)
```

Applying the same steps (iii) - (v) as above, we find that the Type I error rate of both analyses seems similar to what we obtained above:

```{r}
d.type1.boot = 
  plyr::rdply(
    .n = n.samples,
    function(i) {
      dd = try(bootstrap_single_subject_data(n.trial, d.subj) %>%
                 do_test(), silent = TRUE)
      if (any(class(dd) == "try-error"))
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    }) 

d.type1.boot %>%
  get_TypeI()
```


## Power

To calculate power, we proceed in parallel to the approach described above for Type I error rates. For power, too, we can use parametric or non-parametric approaches. The following example shows the parametric approach to simulated the data from Subject ```r which.subj```. We can modify the function used above, so that it can accommodate both Type I error analyses (for effect = 0) and power analyses (for effects != 0). For the sake of simplicity, we focus on only RTs.

```{r}
# A function to parametrically generate data with an effect
make_single_subject_data = function(n.trial, d.pars, effect = 0) {
  crossing(
    subject = 1,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    RT = rnorm(
      nrow(.), 
      # Here we are adding the effect to the mean of the normal 
      # distribution we are sampling from:
      d.pars$RT_mean[1] + ifelse(condition == "low frequency", -effect, +effect),
      d.pars$RT_sd[1])
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition)
}
```

Here's an example draw from this revised function, for a hypothetical (huge) difference of 250 milliseconds between the means of the two conditions:

```{r}
make_single_subject_data(n.trial, d.pars, effect = 250)
```

With this simple change in the data generation---i.e., step (i)---we can calculate the power for, e.g., a combination of different amounts of data (12, 24, 48, or 96 trials) and different effect sizes (25, 50, and 100 ms):

```{r, echo=TRUE}
get_power = . %>%
  get_TypeI() %>%
  rename_all(., .funs = ~gsub("Type_I", "Power", .x))

sample_power = function(d.pars, n.trials, effects, n.samples = n.samples) {
  d = tibble(.rows = 0)
  for (n.trial in n.trials) {
    for (effect in effects) {
      d %<>% rbind(
        plyr::rdply(
          .n = n.samples,
          function(i) {
            dd = try(
              make_single_subject_data(n.trial, d.pars, effect = effect)  %>%
                do_test(), silent = TRUE)
            if (any(class(dd) == "try-error"))
              dd = data.frame(RT.p.value = NA)
            return(dd)
          }) %>%
          mutate(effect = effect, n.trial = n.trial))
    }
  }
  
  d %<>%
    group_by(effect, n.trial) %>%
    dplyr::mutate_at(
    vars(ends_with("p.value")), 
    function(x) x < .05) %>%
    rename_all(.funs = ~ gsub("p\\.value", "Power", .x))

  return(d)
}

temp <- sample_power(
  n.trials = c(12, 24, 48, 96),
  effects = c(25, 50, 100),
  d.pars = d.pars,
  n.samples = 200) 

temp %>%
  ggplot(aes(x = n.trial, y = ifelse(RT.Power, 1, 0), color = factor(effect))) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  stat_summary(fun = mean, geom = "line") +
  # Adding a line to indicate the 'power' we'd get from the Type I error
  # alone
  geom_hline(yintercept = .05, linetype = 2, color = "darkgray") +
  scale_x_continuous("Number of trials") +
  scale_y_continuous("Power", limits = c(0,1)) +
  scale_color_discrete("Effect size") + theme_bw()
```


### Power in grouped data

The same approach can be applied to grouped data. Here is an example using a generation function that is similar to the one used to generate the (simulated) results that we introduced at the beginning of this document. We can then ask how the power of a *paired* $t$-test over the by-participant means of RTs and accuracy scales as a function of the number of trials and the number of subjects. If you are working with similar repeated-measures data, have a look at the code in the R markdown source.

Note how we stand to gain much less power from increasing the number of trials per subjects, compared to increasing the number of subjects (e.g., 4 subjects with 8 trials each is the same amount of total data as 8 subjects with 4 trials each, but the latter gives us more power). Of course, this depends on the relative variability *within* vs. *between* subjects, but the scenario depicted here holds for many of our experiments.

```{r, echo=FALSE}
make_multi_subject_data = function(n.subject, n.trial, 
                                   effect.RT = 0, effect.correct = 0) {
  crossing(
    subject = 1:n.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "low frequency" ~ plogis(qlogis(.8) - effect.correct / 2),
      condition == "high frequency" ~ plogis(qlogis(.8) + effect.correct / 2)
    )),
    muLogRT = case_when(
      condition == "low frequency" ~ log(exp(6.3) + effect.RT / 2),
      condition == "high frequency" ~ log(exp(6.3) - effect.RT / 2)
    )
  ) %>%
  group_by(subject, condition) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, .5),
    muLogRT.bySubject = rnorm(1, 0, .1)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(100 + exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition) %>%
    select(-starts_with("mu"))
}

do_multi_subject_test = function(data) {
  data %<>%
    group_by(subject, condition) %>%
    summarise_at(c("RT", "correct"), mean)
  
  d = tibble(.rows = 1)
  if ("RT" %in% names(data)) {
    t1 = t.test(RT ~ condition, data = data, var.equal = T, paired = T)
    d %<>%
      mutate(RT.p.value = t1$p.value)
  }
  if ("correct" %in% names(data)) {
    t2 = t.test(correct ~ condition, data = data, var.equal = T, paired = T)
    d %<>%
      mutate(Accuracy.p.value = t2$p.value)
  }

  return(d)
}

sample_power = function(n.subjects, n.trials, n.samples = n.samples) {
  d = tibble(.rows = 0)
  for (n.subject in n.subjects) {
    for (n.trial in n.trials) {
      d %<>% rbind(
        plyr::rdply(
          .n = n.samples,
          function(i) {
            dd = try(
              make_multi_subject_data(n.subject, n.trial, 
                                      effect.RT = 25,
                                      effect.correct = 1)  %>%
                do_multi_subject_test(), silent = TRUE)
            if (any(class(dd) == "try-error"))
              dd = data.frame(RT.p.value = NA)
            return(dd)
          }) %>%
          mutate(n.subject = n.subject, n.trial = n.trial))
    }
  }
  
  d %<>%
    group_by(n.subject, n.trial) %>%
    get_power()
  
  return(d)
}

sample_power(n.subjects = c(4, 8, 16, 32), n.trials = c(4, 8, 16, 32), 
             n.samples = 300) %>%
  pivot_longer(
    cols = c("RT.Power", "Accuracy.Power"),
    values_to = "Power",
    names_pattern = "(.*)\\.Power",
    names_to = "DV"
  ) %>%
  mutate(DV = factor(DV, levels = c("RT", "Accuracy"))) %>%
  ggplot(aes(x = n.subject, y = Power, color = factor(n.trial))) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", alpha = .5) +
  stat_summary(fun = mean, geom = "line", alpha = .5) +
  # Adding a line to indicate the 'power' we'd get from the Type I error
  # alone
  geom_hline(yintercept = .05, linetype = 2, color = "darkgray") +
  scale_x_continuous("Number of subject") +
  scale_y_continuous("Power", limits = c(0,1)) +
  scale_color_discrete("Number\nof trials") + theme_bw() +
  facet_wrap(~DV)
```

# Researcher's degrees of freedom: a sampling-based case study

Let's say we collect 30 subjects' worth of data (with 24 trials each), from the same hypothetical experiment used in the power analyses right above. We then conduct a paired $t$-test on both the RTs and the accuracy:

```{r}
d = make_multi_subject_data(n.trial = 16, n.subject = 24) 

d %>%
  do_multi_subject_test()
```

The result of the $t$-test for the reaction times looks 'promising'. They are significant, and one might say it is "approaching significance". So we decide that we need more statistical power to understand the effect. We collect data from 8 more subjects and then test again:

```{r}
d %<>%
  rbind(
    make_multi_subject_data(n.trial = 16, n.subject = 8))

d %>%
  do_multi_subject_test()
```

It seems like we were on the right track! There is indeed a significant effect of high vs. low frequency on reaction times. We decide to submit the result for publication. 

Eight years later, a team of undergraduate researchers decides to replicate our study. They succeed in following the same design, procedure, etc., and sample from the same underlying population. They are weary of the small number of trials and subjects. They decided to run twice the number of trials per subject (32) and four times the number of subjects of the original study (128), for a total of *eight times the original data*. Using the same paired $t$-test approach, they find:

```{r}
make_multi_subject_data(n.trial = 32, n.subject = 128) %>%
  do_multi_subject_test()
```

Which study should we trust? A failure to replicate is *not necessarily* due to any fault of the original researchers (or the replication team): even if the test we employ achieves the targeted Type I error rate of 5%, one in twenty experiments will return a significant effect even if the underlying effect is null. 

But is there something that the original team could have done better? If you don't know the answer, make sure to read Nelson et al. (2008) article in the *Annual Review of Psychology*.

## Illustrating the consequences of the run-test-run-test-... scheme (incremental testing)

Let's use the sampling-based approach to investigate the consequences of this incremental approach. We first write a function that collects data in 10 steps (of 10 subjects with 16 trials each), each time testing all the data collected so far. 

In the output, we see the $p$-values for each batch of 10 subjects, both for the $t$-test on RTs and the $t$-test on accuracy. Next to it, we see the column `RT.significant`, indicating whether the $p$-value for the RT analysis was significant (< .05) *in that batch*. Next to that, the columns `RT.significant.incremental` tells us whether the $p$-value *in this or any preceding batch* was significant. This captures the **incremental testing** approach, in which we would stop any time we have found significance. For example, even if I had planned to in theory collect data from up to 10 batches of 10 subjects (testing for an effect after each batch has been collected), under the incremental testing approach I'd stop as soon as I found significance on any of the earlier batches. I'd then conclude and report a significant effect. That's why the column `RT.significant.incremental` has the value `TRUE` for any batch that follow a batch on which I have found significance.

```{r}
run_test_run_test = function(n.trial = 16, n.subject = 10, n.batch = 10) {
  d = plyr::rdply(
    .n = n.batch, .id = "batch",
    make_multi_subject_data(n.trial = n.trial, n.subject = n.subject)) %>%
    group_by(batch, subject, condition) %>%
    dplyr::summarise_at(c("correct", "RT"), mean)
  
  d.t = tibble(.rows = 0)
  for (i in 1:max(d$batch)) {
    d.t %<>%
      rbind(
      do_multi_subject_test(d %>%
                              filter(batch <= i)) %>%
        mutate(batch = i))
  }
  
  d.t %>%
    mutate(
      RT.significant = RT.p.value < .05,
      RT.significant.incremental = cumany(RT.significant),
      Acc.significant = Accuracy.p.value < .05,
      Acc.significant.incremental = cumany(Acc.significant)
      )
}

run_test_run_test()
```

Now we can ask what happens if we repeated this thought experiment many times. The black line in the following plot shows the estimated Type I error rate if we plan how many subjects we run and only test once (e.g., we might plan to run 6 batches, i.e., 60 subjects and then test and stop, regardless of the result we find). The blue line shows the Type I error rate if we test after every batch of subject and stop if we find significance.

```{r}
d.sim = plyr::rdply(
  .n = 500,
  run_test_run_test()
) 

d.sim %>%
  group_by(.n, batch) %>%
  summarise(
    RT.TypeI.cumulative = mean(RT.significant),
    Acc.TypeI.cumulative = mean(Acc.significant),
    RT.TypeI.incremental = mean(RT.significant.incremental),
    Acc.TypeI.incremental = mean(Acc.significant.incremental)) %>%
  ggplot(aes(x = batch, y = RT.TypeI.incremental)) +
  stat_summary(fun = mean, geom = "line", color = "blue") +
  stat_summary(fun = mean, geom = "line", color = "black", aes(y = RT.TypeI.cumulative)) +
  # Adding a line to indicate the 'power' we'd get from the Type I error
  # alone
  geom_hline(yintercept = .05, linetype = 2, color = "darkgray") +
  scale_x_continuous("Number of batches of subjects (10 each) administered") +
  scale_y_continuous("Type I error", limits = c(0,1)) + theme_bw()
```

This illustrates the problem---an inflated Type I error---of the incremental testing approach. Simmons et al. (2011) in *Psychological Science*, summarized also in the Nelson et al. (2018) article, discuss incremental testing as one of several common practices and researchers' degrees of freedom that have under-appreciated consequences on the Type I error of our studies.
