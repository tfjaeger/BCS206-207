---
title: 'Confidence intervals'
author: "Florian Jaeger"
date: "Fall 2020"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
urlcolor: blue
header-includes:
   - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, results = "markup", warning = TRUE, cache = FALSE, message = FALSE,
  fig.align = "center", fig.width = 6.5)
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("boot")       # for automatic bootstrap

theme_set(theme_bw())
```


# Goals of this document
This document is intended to introduce the basics of confidence intervals. 

## In preparation for class
Before class, please read the remainder of Section 1. Then watch this great [video on confidence intervals by Dr. Nic ](https://www.youtube.com/watch?v=tFWsuO9f74o). Then watch this simulation video by the Khan Academy on the [meaning of confidence intervals](https://www.khanacademy.org/math/ap-statistics/estimating-confidence-ap/introduction-confidence-intervals/v/confidence-interval-simulation) and what happens to those confidence intervals, if we repeat (replicate) an experiment. 

You might prefer to instead (or also) have a manuscript that covers this topic in writing. Cumming's (2013) [suggestions for a 'new' statistics](https://journals.sagepub.com/doi/full/10.1177/0956797613504966) is a worthwhile read and has a self-contained section on "Replication, p values, and CIs" that I recommend. You can just read those 2 pages.


## Important concepts and terminology
In the brain and cognitive sciences, the goal of conducting experiments is to advance our understanding of the brain/mind. We aim to test the **predictions** of **theories** or more specific **hypotheses**. These theories typically constitute propositions about causes, and the statistical tests we conduct typically assess the predicted correlations that would result from the hypothesized causal relations. These correlations are assessed on some finite amount of data from *samples* we've obtained from the underyling *population* of interest.

To illustrate this with an example, remember the experiment on visual crowding introduced in a [previous lecture](https://github.com/tfjaeger/BCS206-207-R-lectures/blob/2021-2022/scripts/R/3a-Data-wrangling-and-visualization.pdf). To recap that experiment briefly:

> On each trial, subjects had to fixate on a fixation cross. The fixation cross was then replaced by a small small number, and subjects had to quickly answer whether they saw the number "3" or "8" (making this a two-alternative forced choice, 2AFC). Half of the trials displayed "3", half displayed "8". Within subject, the experiment manipulated two variables, the size of the number (1, 2, or 4) and whether the number was surrounded by four other numbers above, below, left, and right of it (hard) or not (easy). The presence of other numbers presented close to the target number is known to make recogntion of the target number harder---an effect known as "visual crowding". For each trial, the software recorded whether the subject's response was correct (1 = correct; 0 = incorrect) and the reaction time for the response (RT).

We hypothesized that (H1) performance in a number recognition task increases with increasing size of the number and that (H2) performance decreases in visually crowded displays. We elicited two types of **dependent variables** or **outcomes**: response time and response accuracy. To turn our hypotheses into predictions, we implicitly assumed **linking hypotheses** that link the abstract theoretical construct ("performance") to the specific dependent measures we employ in our experiment: higher performance $\rightarrow$ faster processing $\rightarrow$ response times and higher performance $\rightarrow$ higher accuracy. 

To test the resulting predictions, we manipulated both the number size (size 1, 2, 4) and the visual crowding (easy: no crowding, hard: numbers flanking the target number on all sides). Thus size and crowding are our **independent variables** or **predictors**.

When we analyze data from our experiment, we assume that the **observations** we obtain constitute a **sample** of the underlying **population**. We then use this finite sample to draw **inferences** about the population. For example, a really typical question we ask would be whether the means of the population differ between two conditions. Sticking with our example, we might test our hypothesis about the brain/mind by asking whether we can confidently conclude that reaction times are indeed smaller in for high frequency objects, compared to reactions times for high frequency objects. Either way, this constitutes an inference we're aiming to draw about the world, based on the finite sample of observations we collected in our experiment.

Specifically, we will compare the condition **means in our sample**. For example to test (H2), we would compare the mean response time in the easy and hard condition of the experiment. In null hypothesis significance testing (NHST), we do this comparison by **assuming the null hypothesis that there is *no* difference** and then assessing how improbable the actual sample means are under that null hypothesis. If the sample means are too far apart from each other so that their difference is improbable to have happened by chance under the null hypothesis, we **reject the null hypothesis** and speak of a **significant difference**. 

But how do we determine how different is "too different", so that we should reject the null hypothesis? For  this, we need a measure of our **uncertainty**. Intuitively, this measure should depend on the inherent **random variability** of the data: the more variable the data is---i.e., the more each sample drawn from the underlying population will differ from each other---the less certain we can be that the same absolute difference between two condition means is meaningful (or in NHST terms: "significant"). This is where confidence intervals come in.

## Confidence intervals

Confidence intervals are an effective means to summarize our uncertainty. From Cohen (1994, p. 1002) as quoted on the [ReplicationIndex blog](https://replicationindex.com/tag/dancing-p-values/):

\begin{quote}
"Everyone knows" that confidence intervals contain all the information to be found in significance tests and much more. They not only reveal the status of the trivial nil hypothesis but also about the status of non-nil null hypotheses and thus help remind researchers about the possible operation of the crud factor. Yet they are rarely to be found in the literature. I suspect that the main reason they are not reported is that they are so embarrassingly large! But their sheer size should move us toward improving our measurement by seeking to reduce the unreliable and invalid part of the variance in our measures (as Student himself recommended almost a century ago). Also, their width provides us with the analogue of power analysis in significance testingâ€”larger sample sizes reduce the size of confidence intervals as they increase the statistical power of NHST.
\end{quote}

Confidence intervals on the one hand are rather intuitive---in that they provide a measure of variability or uncertainty---but the specifics of confidence intervals are **often misunderstood and misinterpreted**. The following [summary of common misunderstandings](http://www.timvanderzee.com/not-interpret-confidence-intervals/) provides a concise overview. The somewhat unintuitive interpretation of CIs is that---provided they are constructed properly---95% of all 95% CIs over repeated samples from a population will contain the true population mean.




# Illustrating the meaning of confidence intervals

To illustrate the notion of CIs let's go through a simple example. For this purpose, we will choose an example where we know the 'ground truth'---unlike in typical research. That is, we will generate our own data. Specifically, imagine that we have a normally distributed outcome with mean 10 and standard deviation 2. We take 20 samples from that distribution. Here's one instance of this thought experiment. The blue line shows the normal distribution we are sampling from. The tick marks show the 20 data points we sampled. The black line is a density estimate of the 20 data points.

```{r, fig.height=3, fig.width=4}
# set a random seed (so that repeated instances of this document yield the same result)
set.seed(9)

# make a table of 20 rows and store 20 random draws from a normal distribution with mean 10 and sd 2
t = tibble(y = rnorm(n = 20, mean = 10, sd = 2))

ggplot(
  data = t,
  aes(x = y)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = "blue") +
  geom_rug() +
  scale_x_continuous(limits = c(0,20))
```

Here are 12 more instances of this thought experiment (the blue line is always the same, as it shows the *population's distribution*). This time I've also plotted a 95% CI based on the (in this case, correct) assumption of a normally distributed outcome. We'll learn below how it was constructed.

```{r, message=F}
# set a random seed (so that repeated instances of this document yield the same result)
set.seed(5)
n.exp = 12

# make a table of 20 rows and store 20 random draws from a normal distribution with mean 10 and sd 2
t = tibble(
  Experiment = sort(rep(1:n.exp, 20)),
  y = rnorm(n = 20 * n.exp, mean = 10, sd = 2))

ggplot(
  data = t,
  aes(x = y)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = "blue") +
  geom_rug() +
  geom_segment(
    data = . %>% 
      group_by(Experiment) %>%
      summarise(
        mean = mean(y),
        se = sd(y) / sqrt(length(y))
      ),
    y = 0, yend = 0,
    aes(
      x = mean - 1.98 * se, 
      xend = mean + 1.98 * se,
      color = mean - 1.98 * se < 10 & mean + 1.98 * se > 10),
    size = 1
  ) +
  scale_x_continuous(limits = c(0,20)) +
  facet_wrap(~ Experiment) +
  scale_color_manual(
    "95% CI contains true mean?",
    breaks = c(T, F),
    labels = c("yes", "no"),
    values = c("green", "red")
  ) +
  theme(legend.position = "bottom")
```

To focus on the CIs, let's just plot the 20 observations and the CI for each instance of the experiment. We'll essentially make the x-axis of the previous plot the y-axis of the new plot, and use the x-axis to indicate the different experiments. That way we can plot many more thought experiments next to each other. Let's plot 200 of them. The CI is colored green if it includes the true mean and red if it doesn't.

```{r, fig.width=12.5, message=F}
# set a random seed (so that repeated instances of this document yield the same result)
set.seed(5)
n.exp = 200

t = tibble(
  Experiment = sort(rep(1:n.exp, 20)),
  y = rnorm(n = 20 * n.exp, mean = 10, sd = 2))

ggplot(
  data = t,
  aes(x = Experiment, y = y)) +
  geom_point(alpha = .5) +
  geom_segment(
    data = . %>% 
      group_by(Experiment) %>%
      summarise(
        mean = mean(y),
        se = sd(y) / sqrt(length(y))
      ),
    aes(
      y = mean - 1.98 * se, 
      yend = mean + 1.98 * se, 
      xend = Experiment,
      color = mean - 1.98 * se < 10 & mean + 1.98 * se > 10),
    size = 1
  ) +
  geom_hline(yintercept = 10, color = "blue") +
  scale_x_continuous(expand = c(0,0)) +
  scale_color_manual(
    "95% CI contains true mean?",
    breaks = c(T, F),
    labels = c("yes", "no"),
    values = c("green", "red")
  ) +
  theme(legend.position = "bottom")
```
This begins to give us an intuition as to what it means that 95% of our 95% CIs over repeated experiments are guaranteed to contain the true mean of the data. We see 7 red lines out of 200 (3.5%), so that 96.5% of the 200 CIs we have constructed contain the true mean. If we continue to collect more samples we will find that in the long run 95% of the 95% CIs contain the true mean.

Of course, for a real-life example we won't know the true mean. But we will still know that if we repeat the experiment over and over again then 95% of the 95\% CIs will contain the true mean.

# How to obtain confidence intervals

## Parametric confidence intervals

When the distribution of a variable is known---or if we are willing to make assumptions about that distributions---there might be a known analytic solution to calculating the confidence intervals. For example, under the assumption that an outcome $y$ are normally distributed around an unknown mean $\mu$ the 95% CI around the *sample* mean $mean_y$ is:

$mean_y \pm 1.96 * \frac{SD_y}{\sqrt{n}}$

where $SD_y$ is the *sample's* standard deviation of y---i.e., the standard deviation of $y$ in our observed sample. Why is it $\pm 1.96$? Because 95% of the density of a normal distribution lie between its mean $\pm 1.959964...$ its standard deviation---i.e., between its .025 and .975 quantile. E.g. for our example from above, a normal distribution with mean 10 and standard deviation 2:

```{r, warning=FALSE, fig.height=3, fig.width=4}
mean = 10
sd = 2

ggplot(
  data = tibble(x = seq(0, 20, .1)),
  aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), color = "blue") +
  geom_area(
    data = . %>%
      mutate(
        x = 
          ifelse(
            between(
              x, 
              qnorm(.025, mean = mean, sd = sd),
              qnorm(.975, mean = mean, sd = sd)), 
            x, NA),
        y = dnorm(x, mean = mean, sd = sd)),
    aes(y = y), fill = "blue", alpha = .5) +
  scale_x_continuous("value") +
  scale_y_continuous("density")
```

This approach to CIs is called parametric because the CIs are obtained under the assumption of a specific parametric distribution. For example, the normal distribution is a parametric distribution, a distribution specified by parameters (the mean and the standard deviation).

Notice two important properties of the CIs defined above, both due to the term $\frac{SD_y}{\sqrt{n}}$ (the so-called **standard error of the sample mean**): 

 * the more variable the observation in the sample ($SD_y$), the larger/wider the CIs
 * the more sample data we have, the smaller/more narrow the CIs
 
To make this more concrete, consider the experiment from last week. Let's calculate the 95% CIs for the six conditions defined by crossing the three number sizes and two crowding conditions. We'll do so for subject 1.

```{r, echo=T, message=F}
d = read_csv("../../data/example_data.csv")

d.summary =  d %>%
  filter(subject == 1) %>%
  group_by(size, condition) %>%
  summarise_at(
    .vars = "RT", 
    .funs = list(
      "mean" = function(x) mean(x),
      "CI.lower" = function(x) { mean(x) - 1.96 * sd(x) / sqrt(length(x) - 1) },
      "CI.upper" = function(x) { mean(x) + 1.96 * sd(x) / sqrt(length(x) - 1) }))

print(d.summary)
```

Or plotted:

```{r}
d.summary %>%
  ggplot(aes(x = size, 
             y = mean, 
             ymin = CI.lower,
             ymax = CI.upper,
             color = condition)) +
  geom_pointrange(position = position_dodge(.1)) +
  geom_line(position = position_dodge(.1))
```

## Issues with parametric confidence intervals

One issue with parametric CIs becomes apparent when we make parametric assumptions that are *wrong*. For example, the correctness of the responses in the visual crowding data follows a Bernoulli, rather than Normal, distribution. If we obtain CIs for the mean proportion of correct responses in the six conditions following the procedure we used for the RTs, we get the following: 

```{r}
d %>%
  filter(subject == 1) %>%
  group_by(size, condition) %>%
  summarise_at(
    .cols = c("correct"), 
    .funs = list(
      "mean" = function(x) mean(x),
      "CI.lower" = function(x) { mean(x) - 1.96 * sd(x) / sqrt(length(x) - 1) },
      "CI.upper" = function(x) { mean(x) + 1.96 * sd(x) / sqrt(length(x) - 1) })) %>%
  ggplot(aes(x = size, 
             y = mean, 
             ymin = CI.lower,
             ymax = CI.upper,
             color = condition)) +
  geom_pointrange(position = position_dodge(.1)) +
  geom_line(position = position_dodge(.1)) +
  scale_y_continuous("Proportion correct answers") +
  coord_cartesian(ylim = c(0.75,1))
```

Note how the 95% CIs include values outside of the possible range for proportions (larger than 1 in this case). This is a clear indication that we cannot safely assume normality for this type of data. There are generally two ways around this issue: either we obtain CIs under assumptions that reflect the actual generative process underlying the data, or we use non-parametric methods to obtain CIs that do not assume a specific parametric form of the distribution. The second approach has the advantage that it works for any type of distribution, which is great since the true distribution of the outcome is typically not known!

## Non-parametric confidence intervals

One non-parametric method to calculate CIs is the **bootstrap**. There are many different implementations of it, and the details can become quite complicated depending on the type of data you are analyzing. At the core though, the bootstrap is based on a rather intuitive idea. Rather than to make parametric assumptions about the distribution of the data and the resulting expected distribution of the sample means (across samples), we can use the sample data we have to estimate the distribution. Specifically, we can randomly sample with replacement a new sample *from the sample data we have*. For example, for subject 1 from the same experiment as above we resample 720 new trials (balanced in the same way as before) from the 720 trials we have from that subject:

```{r}
set.seed(9876)

d.sample = d %>%
  filter(subject == 1) %>%
  group_by(size, condition) %>%
  # This resamples with replacement a new set of data from the old set of data
  # Setting size to 1 means that the new set of data has the same number of data 
  # points ('is of the same size') as the old data 
  sample_frac(size = 1, replace = T)

d.sample
```

Note that these are not the same trials as before. We can see that by plotting a histogram of the original trial ID in the newly resampled data. We can see that some trials were sampled multiple times, and others were sampled never:

```{r}
ggplot(
  d.sample,
  aes(x = trial)) +
  geom_bar() + 
  facet_grid(condition ~ size)
```

Now we can obtain the condition means from this resampled data. By repeating this process many times, we can thus obtain an estimate of the *distribution of the condition means* across random samples. For example, from 1000 random resamples of the data, we obtain 1000 means for each of the six conditions. Here I'm illustrating this for the mean proportion of correct responses for which we have seen above that the assumption of normality is problematic:

```{r, message=F}
d.sample_means = d %>%
  filter(subject == 1) %>%
  # duplicate the data 1000 times
  crossing(sample = 1:1000) %>%
  group_by(sample, size, condition) %>%
  # This resamples with replacement a new set of data from the old set of data
  # Setting size to 1 means that the new set of data has the same number of data 
  # points ('is of the same size') as the old data 
  sample_frac(size = 1, replace = T) %>%
  summarise(sample_mean = mean(correct))

print(d.sample_means)
```
Now we simply ask what the 2.5th to 97.5th quantiles of each of these six distribution of means are. Basically, we determine the range around the mean in which 95% of the resampled means fall. Notice that the CIs based on these quantiles do not (and can never) fall outside of the possible range of the data. Notice also that the CIs are asymmetric around the mean, as we would expect because precisely because they should not include values outside of the [0,1] range:

```{r, echo=T}
d.sample_summary = d.sample_means %>%
  group_by(size, condition) %>%
  summarise(
    mean = mean(sample_mean),
    CI.lower = quantile(sample_mean, .025),
    CI.upper = quantile(sample_mean, .975)
  )

print(d.sample_summary)
d.sample_summary %>%
  ggplot(aes(x = size, 
             y = mean, 
             ymin = CI.lower,
             ymax = CI.upper,
             color = condition)) +
  geom_pointrange(position = position_dodge(.1)) +
  geom_line() +
  scale_y_continuous("Proportion correct answers") +
  coord_cartesian(ylim = c(0.75,1))
```

Compare this to the CIs obtained from the bootstrap function built into ggplot:

```{r, echo=T}
d %>%
  filter(subject == 1) %>%
  ggplot(aes(x = size, 
             y = correct,
             color = condition)) +
  stat_summary(
    fun.data = mean_cl_boot, 
    geom = "pointrange", 
    position = position_dodge(.1)) +
  stat_summary(
    fun.data = mean_cl_boot, 
    geom = "line", 
    position = position_dodge(.1)) +
  scale_y_continuous("Proportion correct answers") +
  coord_cartesian(ylim = c(0.75,1))
```

Bootstrap is a powerful (and ultimately simple) option for obtained confidence intervals. One important limitation to keep in mind about the bootstrap though is that it is utterly dependent on the representativeness of the sample data. One direct consequence of that is that bootstrapped CIs become more robust the more sample data we have. If you're interested in learning more about bootstrap and sampling-based approaches to statistics, check out the lecture on sampling-based approaches in the same [Github repo that this lecture come from](https://github.com/tfjaeger/BCS206-207-R-lectures/tree/2021-2022/scripts/R).

# Confidence intervals for grouped (repeated measures) data

So far we have obtained CIs only for data from a single subject. When we have repeated measures from multiple subjects, we can still use the approaches discussed here. We should, however, apply them to the *by-subject means* rather than the raw data to obtain visualizations that approximate appropriately conservative (i.e., wide) CIs. 

Consider, for example, that we would like to plot the CIs for the same six conditions analyzed above but would like to do so for all subjects together rather than just subject 1. In that case, we should first summarize the data down to one mean for each of the six conditions for each subject, and then obtain CIs for the distribution of these by-subject means. Notice how the CIs are now considerably larger. That is because they now reflect our uncertainty about not just subject 1 but about the mean across *all* subjects (and we are more uncertain at this level of the data because subjects vary substantially from each other in this example---as is also typically the case in many real experiments).

```{r, echo=T}
d %>%
  # the next two lines first create by-subject means for each of the six conditions
  group_by(subject, condition, size) %>%
  summarize(correct = mean(correct)) %>%
  # the rest of this code is the same as above
  ggplot(aes(x = size, 
             y = correct,
             color = condition)) +
  stat_summary(
    fun.data = mean_cl_boot, 
    geom = "pointrange", 
    position = position_dodge(.1)) +
  stat_summary(
    fun.data = mean_cl_boot, 
    geom = "line", 
    position = position_dodge(.1)) +
  scale_y_continuous("Proportion correct answers") +
  coord_cartesian(ylim = c(0,1))
```

# Confidence intervals around the mean vs. around *differences* in a pair of means

Plots of condition means like the ones we have seen above are very common in scientific papers. They provide intuitive summaries of the raw data. But how do the confidence intervals of the condition means relate to the statistical tests we conduct? Can we infer from a plot of condition means and CIs whether there is a significant effect? The short answer is "kind of" but "not really".

The reason for that is that our statistical tests typically assess hypotheses about *differences between means*, rather than the means themselves. For example, if want to know whether RTs in the the easy (uncrowded) condition were significantly faster than RTs in the hard (crowded) condition we are testing whether we can reject the null hypothesis that the *difference* in RTs between these conditions is 0. It is therefore the CI of the mean difference between the conditions that tells whether there is an effect. For the conventional sigificance criterion of .05, we would reject the null hypothesis if the 95% CI around the mean difference in RTs between the easy and hard condition does not contain 0.

So unless we plot difference between RTs, rather than the mean RTs of all conditions, the CIs in the plot do not directly map onto the p-values of our statistical tests. That said, especially when we're not dealing with paired data (i.e., if the data aren't grouped) significant differences in the means will cause the CIs around a means to not include the other mean.

# Session info
```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
